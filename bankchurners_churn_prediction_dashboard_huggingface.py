# -*- coding: utf-8 -*-
"""BankChurners_Churn_Prediction_Dashboard_HuggingFace.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vuJKaE11WmpXVgo0-Rc03lLeOvSiaFar
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

dataset_check = pd.read_csv('BankChurners.csv')

dataset_check.head()

dataset_check.info()

dataset_check.isnull().sum()

dataset_check.describe()

dataset_check=dataset_check.drop(dataset_check.iloc[:,21:],axis=1)

dataset_check.head()

dataset_check['Attrition_Flag'].unique()

dataset_check.shape

dataset_check.columns

df_all=dataset_check.drop_duplicates()

df_all.drop_duplicates().shape
#it means the dataset had no duplicates

#if dataset had duplicates we could remove it like this
#df_all = df_all.drop_duplicates(subset=None , keep='first')
#but we had no duplicates so no need to try this method at any column

import matplotlib.pyplot as plt
plt.figure(figsize=(5,3))



ax=sns.histplot(data=df_all,
               x='Customer_Age',
               bins=range(df_all['Customer_Age'].min(),df_all['Customer_Age'].max())
               )

for p in ax.patches:
    height=p.get_height()
    ax.annotate(
        f'{height}',
        (p.get_x()+p.get_width()/2,height),
        ha='center',va='bottom',
        xytext=(0,2),
        textcoords='offset points',
        fontsize=7,
        rotation=90,

    )

df_all['Credit_Limit'].dtype

#create automatic one function for histograms for labelling median dotted line with linestyle=":" also in the histogram
def histoplotting(column_name,hue_=None):

    if df_all[column_name].dtype == 'float64':
        df_all[column_name]=df_all[column_name].astype('int64')

    plt.figure(figsize=(6,3))

    maximum = df_all[column_name].max()
    minimum = df_all[column_name].min()

    median = df_all[column_name].median()

    if hue_:
        ax=sns.histplot(x=df_all[column_name],
                        bins=range(minimum,maximum+1),
                        hue=hue_,
                        multiple='dodge'
                       )
    else:
         ax=sns.histplot(x=df_all[column_name],
                        bins=range(minimum,maximum+1)
                        )


    #labelling the bars in the histogram
    #running a for loop for each bar

    for p in ax.patches:
        height = p.get_height()
        ax.annotate(
            f'{height}',
            (p.get_x()+p.get_width()/2,p.get_height()),
            xytext=(0,4),
            textcoords='offset points',
            va='bottom',ha='center',
            rotation=90,
            fontsize=7
        )
    #outside the for loop as median is singular
    #represent the axv line using dotted red color line for the representation of median in the particular column

    plt.axvline(x=median, linestyle=":",color='red',linewidth=2)
    #for labelling median axvline using plotlib
    plt.text(median, plt.ylim()[0]-0*plt.ylim()[1],f'Median={median}',color='red',ha='center',va='top')
    #plt.ylim() return tuple(ymin,ymax) in order to  get just upper limit of y axis we use plt.ylim()[1]


    plt.show()

histoplotting('Customer_Age')

df_all.head()

df_all['Avg_Utilization_Ratio'] = df_all['Avg_Utilization_Ratio'].round(1)
df_all.head()

bins= pd.cut(df_all['Customer_Age'],bins=4)
labels_tuples = bins.cat.categories
labels_list = list(f'{int(left)}-{int(right)}'for left,right in zip(labels_tuples.left,labels_tuples.right))


#intorduced a new column in the dataframe df_all
df_all['Customer_Age_Seg'] = pd.cut(df_all['Customer_Age'],bins=4,labels=labels_list)
df_all['Customer_Age_Seg'].head()

df_all.head()

df_income_age = df_all.groupby(['Income_Category','Customer_Age_Seg'])['Customer_Age_Seg'].value_counts().reset_index()

barplotting(df_income_age,'Income_Category','count','Customer_Age_Seg')

plt.figure(figsize=(7,5))


colors = ['blue','green','red','yellow']
sns.histplot(data=df_income_age,
           x='Income_Category',
           hue='Customer_Age_Seg',
           weights='count',
               shrink=0.8,
           palette=colors,
           multiple='stack')

plt.show()

barplotting(df_all,'Marital_Status','Avg_Utilization_Ratio')

#Segment customers by income level and analyze spending patterns or churn rates.
df_churn_income = df_all.groupby('Income_Category')['Attrition_Flag'].value_counts().reset_index()
df_churn_income

barplotting(df_churn_income,'Income_Category','count','Attrition_Flag')

#Exploring the transaction frequency of each customer and based on dependent_count as a hue
order = [0,1,2,3,4,5]
df_all['Dependent_count'] = pd.Categorical(df_all['Dependent_count'],categories=order,ordered=True)

plt.figure(figsize = (10,5))


minimum = df_all['Total_Trans_Ct'].min()
maximum = df_all['Total_Trans_Ct'].max()

median = df_all['Total_Trans_Ct'].median()
ax=sns.histplot(data = df_all,
            x='Total_Trans_Ct',
            bins=range(minimum,maximum+1,10),
            hue='Dependent_count',
            multiple='dodge'
               )

#outside the for loop as median is singular
#represent the axv line using dotted red color line for the representation of median in the particular column

plt.axvline(x=median, linestyle=":",color='red',linewidth=2)
#for labelling median axvline using plotlib
plt.text(median,plt.ylim()[1]*0.9,f'Median={median}',color='red',ha='center',va='center')
#plt.ylim() return tuple(ymin,ymax) in order to  get just upper limit of y axis we use plt.ylim()[1]
plt.xlabel(range(minimum,maximum+1,10))


plt.show()

#creating seperate column for transaction count categories and then creating a dataframe for dependents vs trans ct.
#creating a stack histplot for this purpose

bins_edges = pd.cut(df_all['Total_Trans_Ct'],bins=4).cat.categories
#picking out list of categories
labels = list(f'{int(left)}-{int(right)}'for left,right in zip(bins_edges.left,bins_edges.right))

df_all['Trans_Ct_Seg'] = pd.cut(df_all['Total_Trans_Ct'],bins=4,labels=labels)

#creating a datframe to calculate the total recordings in the dataset for the transaction counts categories
#taking depemdent count as a hue

df_trans_dep = df_all.groupby(['Trans_Ct_Seg','Dependent_count'])['Dependent_count'].value_counts().reset_index()
df_trans_dep.head()

#create a histplot with count as a weights and dependent count as a hue
plt.figure(figsize=(5,5))

sns.histplot(data=df_trans_dep,
            x='Trans_Ct_Seg',
            hue='Dependent_count',
            weights='count',
            shrink=0.7,
            multiple='stack')

plt.title('Transactions VS Dependents of a Family')
plt.show()

order=[ 'Unknown','Less than $40K','$120K +', '$40K - $60K', '$60K - $80K', '$80K - $120K']

df_all['Income_Category'] = pd.Categorical(df_all['Income_Category'], categories=order)

"""Average Credit Limit: Analyze the average credit limit by customer segment (e.g., by income, age group, or gender)."""

#Grouping the avg credit limits according to income categories but remember to analyze AVG
#Analyzing according to hue married/single
df_income_vs_credit = df_all[['Income_Category','Credit_Limit','Marital_Status']].groupby(['Income_Category','Marital_Status'])['Credit_Limit'].mean().reset_index()
df_income_vs_credit['Credit_Limit'] = df_income_vs_credit['Credit_Limit'].round(1)

df_income_vs_credit

#Now showing this above dataframe through bar plot giving marital status as a hue
barplotting(df_income_vs_credit,'Income_Category','Credit_Limit','Marital_Status')

#and creating without hue
barplotting(df_income_vs_credit,'Income_Category','Credit_Limit')

#Observe the male/female category in each eduaction level criterion
#for this purpose creating a seperate dataframe so that no complexity is to be faced

df_edu_gender = df_all.groupby('Education_Level')['Gender'].value_counts().reset_index()

barplotting(df_edu_gender,'Education_Level','count','Gender')

#lets find out males or females have spent more months on book
#obviously the population of females is more but what if unexpectedtly males might have spent more months on book

#we will be just calculating the average months on book for males and females
df_avg_months = df_all[['Gender','Months_on_book']].groupby('Gender').agg({'Months_on_book':'mean'}).reset_index()
#df_avg_months

#displaying it in the form of pie chart
plt.figure(figsize=(3,3))
labels=df_avg_months['Gender'].unique()
plt.pie(df_avg_months['Months_on_book'],labels=labels,autopct='%1.1f%%',startangle=145,colors=['pink','skyblue'])
plt.plot()

"""Despite the fact that there were more females approx 5% more than males but still both spent almost equal months on bank."""

df_all['Gender'] = pd.Categorical(df_all['Gender'])

df_gen_count = df_all['Gender'].value_counts().reset_index()
plt.figure(figsize=(3,3))
labels=df_gen_count['Gender'].unique()


plt.pie(df_gen_count['count'],labels=labels,autopct="%1.1f%%",startangle=45,colors=['pink','skyblue'])
plt.plot()

#seeing the categories of education level how many people belong to which category and having Card_Category as a hue
df_all['Education_Level'] = pd.Categorical(df_all['Education_Level'])
df_edu = df_all[['Education_Level','Card_Category']].groupby(['Education_Level','Card_Category']).value_counts().reset_index()
df_edu

#creating a histogram for having card as a hue

plt.figure(figsize=(10,3))

sns.barplot(data=df_edu,
            x='Education_Level',
            y='count',
            hue='Card_Category')

#creating a dataframe for comparing card holders and their attrition flag status regarding their gender
df_att_card = df_all[['Card_Category','Attrition_Flag','Gender']].groupby(['Card_Category','Attrition_Flag','Gender']).value_counts().reset_index()
df_att_card

#creating histogram for comparing card holders and attrition flag but having gender as a hue

plt.figure(figsize=(7,5))

sns.barplot(data=df_att_card,
           x='Card_Category',
           y='count',
           hue='Attrition_Flag',
           palette='pastel')
plt.plot()

#creating a dataframe for male/female holding which card categories
#creating a pie chart for showing the percentages of male female different card category holders

df_all['Card_Category'].unique()

#converting card categories into categorical columns
order = ['Blue','Platinum','Silver','Gold']
df_all['Card_Category'] = pd.Categorical(df_all['Card_Category'],
                                         categories=order,
                                         ordered=True)

df_card =  df_all[['Gender','Card_Category']].groupby(['Gender','Card_Category']).value_counts().reset_index()
df_card

barplotting(df_card,'Card_Category','count','Gender')

#introducing a new quartile column
df_all['Revolving_Bal_Quartile'],q_edges= pd.qcut(df_all['Total_Revolving_Bal'],
                q=4,
                labels=['Q1','Q2','Q3','Q4'],
                retbins=True)
df_all.head()

df_income = df_all[['Education_Level','Income_Category']].groupby(['Education_Level','Income_Category']).value_counts().reset_index()

df_income

barplotting(df_income,'Education_Level','count','Income_Category')

#grouping according to balance quarter ranges and according to hue of education/salary/Attrition Flag Status
df_bal = df_all[['Revolving_Bal_Quartile','Attrition_Flag']].groupby(['Revolving_Bal_Quartile','Attrition_Flag']).value_counts().reset_index()
df_bal

print(q_edges)
#q_edges has my revolving balance categories stored which i will be plotting on axis as plt.x_label

barplotting(df_bal,'Revolving_Bal_Quartile','count','Attrition_Flag')
#plt.xlabel(q_edges)

histoplotting('Months_on_book')

df_all['Months_Inactive_12_mon'].unique()

#creating a mini dataset of counting how many customers were inactive for how many months in the past 12 months inactivation

mini_df = df_all[['Months_Inactive_12_mon','Gender']].value_counts()
mini_df = mini_df.reset_index(name='count')
mini_df = mini_df.sort_values(by='Months_Inactive_12_mon',ascending=True)

mini_df

def barplotting(df,col1,col2,hue_=None):
    plt.figure(figsize=(10,4))

    if hue_:
        ax=sns.barplot(data=df,
                      x=col1,
                      y=col2,
                      hue=hue_,
                      palette='viridis')
    if not hue_:
        ax=sns.barplot(data=df,
                      x=col1,
                      y=col2,
                      #hue=hue_,
                      palette='viridis')

    #labelling purposes
    for p in ax.patches:
        height = p.get_height()
        ax.annotate(
            f'{height:.0f}',
            (p.get_x()+p.get_width()/2,p.get_height()),
            xytext=(0,4),
            textcoords='offset points',
            va='bottom',ha='center',
            rotation=90,
            fontsize=7
        )

    plt.show()

barplotting(mini_df,'Months_Inactive_12_mon','count','Gender')

plt.figure(figsize=(6,3))

ax = sns.barplot(
           x='Months_Inactive_12_mon',
           y= 'count',
           hue='Gender',
          data = mini_df,
    palette = 'viridis'
)


for p in ax.patches:
    height = p.get_height()
    ax.annotate(
        f'{height}',
        (p.get_x()+p.get_width()/2,p.get_height()),
        xytext=(0,4),
        textcoords='offset points',
        va='bottom',ha='center',
        rotation=90,
        fontsize=7
    )

plt.plot()

histoplotting('Months_Inactive_12_mon')

df_all.shape

# # Map 'Attrited Customer' to 1 and 'Existing Customer' to 0
# df_all['Attrition_Flag'] = df_all['Attrition_Flag'].map({
#     'Attrited Customer': 0,
#     'Existing Customer': 1
# })

# # Verify the conversion
# df_all['Attrition_Flag'].unique()

columns_to_drop = df_all.iloc[:,21:]
df_all.drop(columns_to_drop,axis=1,inplace=True)

df_all.columns

# Columns to encode
col = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']

df_all[col] = df_all[col].astype('category')

#apply cat.codes method to make binary + multiple encoding columns

df_all[col] = df_all[col].apply(lambda x: x.cat.codes)
df_all.head()

# Assuming `dataset_check` is the DataFrame with the original categorical values

# Columns to encode (as per your previous code)
col = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']

# Dictionary to store mappings of category labels to codes from `dataset_check`
categorical_mappings = {}

# Retrieve category labels from `dataset_check` before encoding
for column in col:
    # Ensure the column in `dataset_check` is of category dtype
    dataset_check[column] = dataset_check[column].astype('category')

    # Store the category labels and their corresponding numeric codes
    categorical_mappings[column] = {code: category for code, category in enumerate(dataset_check[column].cat.categories)}

# Print the mappings (numeric codes -> category labels)
for column, mapping in categorical_mappings.items():
    print(f"{column} Mapping (Code -> Category):")
    for code, category in mapping.items():
        print(f"Code {code} -> {category}")
    print()

numerical = df_all.select_dtypes(include=['number'])
categorical = df_all['Attrition_Flag']
df_all_features = pd.concat([numerical,categorical], axis=1)
df_all_features.columns

df_all_features.shape

columns_drop  = df_all_features[['CLIENTNUM','Total_Amt_Chng_Q4_Q1','Total_Ct_Chng_Q4_Q1']]
features_df = df_all_features.drop(columns_drop, axis=1)
features_df.head()

features_df.shape

#visualizing outliers in the Customer Age
plt.figure(figsize=(5,3))
sns.boxplot(features_df, x= 'Customer_Age')

"""Deciding to keep the outliers as the customers with old age might be meaningful for predicting churn rate."""

plt.figure(figsize=(5,3))
sns.boxplot(features_df, x= 'Total_Trans_Ct')

# #creating a pair plot to observe relations between different columns
# import matplotlib.pyplot as plt
# import seaborn as sns
# #hue can be set to see the churn and non-churn in two different colours (Exisitng / Attrited Customer)
# plt.figure(figsize=(5,5))
# sns.pairplot(features_df, hue = 'Attrition_Flag')
# plt.show()

#converting Attrited_Flag column into numerical
# Map 'Attrited Customer' to 1 and 'Existing Customer' to 0
features_df['Attrition_Flag'] = features_df['Attrition_Flag'].map({
    'Attrited Customer': 0,
    'Existing Customer': 1
})

# Verify the conversion
features_df['Attrition_Flag'].unique()

"""The two classes are pretty imbalanced non-churners are a minority class.
Even after applying class_weight='balanced' the LogisticRegression is not producing good results.
Hence applying:
#SMOTE TECHNIQUE:
OverSampling the minority class.

Furthermore this sampling technique is applied only on training data.

"""

df = features_df.groupby('Attrition_Flag').size()
print(df)

"""Finding out 10 best features out of 16 to include in our classical ML Models using:


1.RFE (RECUSRSIVE FEATURE ELIMINATION)

2.LassoCV

#RECURSIVE FEATURE ELIMINATION
"""

from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
import joblib

X = features_df.drop('Attrition_Flag', axis=1)
y = features_df['Attrition_Flag']


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
rfe = RFE(estimator=LogisticRegression(), n_features_to_select=17, step=1) #remove 1 feature in each step
rfe.fit(X_scaled, y)

# Get the selected features
selected_features = X.columns[rfe.support_]
selected_features.shape

joblib.dump(selected_features, "selected_features.pkl")

"""for improved and better performance over the class 0 : non-churners.
Include the more amount of columns/features and then try running your model to see the evaluation results.

Like your task is to encode the 'marital status', 'card category', 'income category','education category' such columns.

Make new columns in the dataframe for each of the above mentioned columns as like 'marital status encoded' and include those new columns in the lasso CV for selecting features and then run you model and see the performance.
"""

# for improved and better performance over the class 0 : non-churners.
# Include the more amount of columns/features and then try running your model to see the evaluation results.

# Like your task is to encode the 'marital status', 'card category', 'income category','education category' such columns.

# Make new columns in the dataframe for each of the above mentioned columns as like 'marital status encoded'
# and include those new columns in the lasso CV for selecting features and then run you model and see the performance.

"""simply map the binary categorical columns .
no need to create new columns in the dataframe for such columns.

just like you mapped the' attrited flag ' column .

and encode other categorical columns having more than one categories.
"""

# #using lassoCV for selecting features
# from sklearn.linear_model import LassoCV
# from sklearn.preprocessing import StandardScaler



# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# lasso = LassoCV(cv=17, random_state=42)
# lasso.fit(X_scaled, y)

# selected_features = X.columns[lasso.coef_ != 0]
# selected_features.shape

"""#LOGISTIC REGRESSION

Predicting Churn Rate
"""

!pip install imbalanced-learn

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

#splitting data
X_selected = selected_features
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 42)

#applying SMOTE
X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)

#initializing the LogisticRegression Model

logreg = LogisticRegression(class_weight = 'balanced', C=1.0, random_state = 42)

#fitting the model on SMOTE resampled data

logreg.fit(X_train_resampled,y_train_resampled)

#predecting
y_pred = logreg.predict(X_test)
y_pred_proba = logreg.predict_proba(X_test)

"""#CLASSIFICATION REPORT"""

print(classification_report(y_test,y_pred))

from sklearn.metrics import confusion_matrix
conf = confusion_matrix(y_test,y_pred)

#plotting heatmap
sns.heatmap(conf,annot=True,fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#now checking the churn porbability for a specific client number by the model:Logistic Regression
dataset_check.head() #containing the CLIENTNUM column.

features_df.head() #containing all the features preprocessed or one-hot-encoded

#concatinating the dataset_check's CLIENTNUM column and features_df all columns into one dataframe with the name preprocessed_df.

# Concatenate along columns (axis=1)
preprocessed_df = pd.concat([dataset_check[['CLIENTNUM']], features_df], axis=1)

# Save the dataframe to a CSV for future reference
preprocessed_df.to_csv('preprocessed_df.csv', index=False)

preprocessed_df.head()

def churn_rate_pred(model_name):

  import joblib
  import random

  # Save the selected features used during model training
  imp_features = selected_features # List of 17 feature names used in training

  # Save the model
  joblib.dump(model_name, f'{model_name}_model.joblib')

  # Choose a random CLIENTNUM from the dataframe preprocessed_df
  random_client = random.choice(preprocessed_df['CLIENTNUM'])

  # Filter the row corresponding to the random CLIENTNUM
  client_row = preprocessed_df[preprocessed_df['CLIENTNUM'] == random_client]

  # Drop 'CLIENTNUM' and keep only the selected features
  features_client_num = client_row[imp_features]

  # Reshape the features to convert into a multi-dimensional array
  features_client_num = features_client_num.values.reshape(1, -1)

  #converting it to dataframe to avoid the warning
  features_client_num_df = pd.DataFrame(  features_client_num, columns = imp_features)
  features_client_num_scaled = scaler.transform(features_client_num_df)


  # Load the model from joblib
  model_log_reg = joblib.load(f'{model_name}_model.joblib')

  # Predict churn probability
  prediction = model_log_reg.predict_proba(features_client_num_scaled)
  print(f'Churn rate for CLIENTNUM {random_client}: {(prediction[0][1])*100:.2f}%')
  print(f'Not Churn rate for CLIENTNUM {random_client}: {(prediction[0][0])*100:.2f}%')

"""#Function 'churn_rate_pred(model_name)' will be used for predicting churn rate by other different models for future with the same processed dataset now with the name preprocessed_df"""

churn_rate_pred(logreg)

"""#Z-SCALING TECHNIQUE:
![image.png](data:image/png;base64,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)

#NAIVE-BAYES MODEL:
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib
import random


X = preprocessed_df.drop(columns=["CLIENTNUM", "Attrition_Flag"])
y = preprocessed_df["Attrition_Flag"]

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Save the model
joblib.dump(model, 'naive_bayes_model.joblib')

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

#confusion matrix
from sklearn.metrics import confusion_matrix
conf = confusion_matrix(y_test,y_pred)

#plotting heatmap
sns.heatmap(conf,annot=True,fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show

# Function to predict churn rate for a random CLIENTNUM
def churn_rate_pred_naive_bayes():
    # Save the selected features used during model training
    selected_features = X.columns.tolist()  # List of feature names used in training

    # Choose a random CLIENTNUM from the preprocessed DataFrame
    random_client = random.choice(preprocessed_df['CLIENTNUM'])

    # Filter the row corresponding to the random CLIENTNUM
    client_row = preprocessed_df[preprocessed_df['CLIENTNUM'] == random_client]

    # Drop 'CLIENTNUM' and keep only the selected features
    features_client_num = client_row[selected_features]

    # Reshape the features to convert into a multi-dimensional array
    features_client_num = features_client_num.values.reshape(1, -1)

    # Convert to DataFrame to avoid warnings
    features_client_num_df = pd.DataFrame(features_client_num, columns=selected_features)
    features_client_num_scaled = scaler.transform(features_client_num_df)

    # Load the model from joblib
    model_naive_bayes = joblib.load('naive_bayes_model.joblib')

    # Predict churn probability
    prediction = model_naive_bayes.predict_proba(features_client_num_scaled)
    print(f'Churn rate for CLIENTNUM {random_client}: {(prediction[0][1])*100:.2f}%')
    print(f'Not Churn rate for CLIENTNUM {random_client}: {(prediction[0][0])*100:.2f}%')

# Test the function
churn_rate_pred_naive_bayes()

"""#SUPPORT VECTOR MACHINE"""

preprocessed_df.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import joblib  # Import joblib to save the model and scaler

# Drop irrelevant columns (e.g., CLIENTNUM)
data = preprocessed_df.drop(columns=["CLIENTNUM"])

# Separate features and target variable
X = data.drop(columns=["Attrition_Flag"])
y = data["Attrition_Flag"]

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize and fit the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data using the same scaler
X_test_scaled = scaler.transform(X_test)

# Save the scaler to disk using joblib (so it can be reused later)
joblib.dump(scaler, 'scaler.joblib')

# Train SVM model
svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Save the trained model to disk
joblib.dump(svm_model, 'svm_model.joblib')

# Get prediction probabilities for Churned Class
y_prob = svm_model.predict_proba(X_test_scaled)[:, 1]
# 1 for positive class: Churned class

# Adjust threshold to remove bias/inclination towards one specific class
threshold = 0.6
y_pred = (y_prob >= threshold).astype(int)

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
print(f"SVM Accuracy: {accuracy}")
print("SVM Classification Report:\n", classification_report(y_test, y_pred))

"""#THRESHOLD ADJUSTMENT TO CONTROL BIASNESS:

By setting the threshold to 0.6, the model is being instructed to be stricter in assigning a sample to the positive class ("Churned"). Now, only probabilities greater than or equal to 0.6 will be classified as positive.
"""

# Optional: Confusion matrix and heatmap for better evaluation
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Not Churned", "Churned"], yticklabels=["Not Churned", "Churned"])
plt.title('Confusion Matrix')
plt.show()

# Load the saved scaler
scaler = joblib.load('scaler.joblib')

# Load the saved SVM model
svm_model = joblib.load('svm_model.joblib')

# Example of predicting for a random customer:
selected_features = X.columns.tolist()  # List of feature names used in training
random_client = random.choice(preprocessed_df['CLIENTNUM'])
client_row = preprocessed_df[preprocessed_df['CLIENTNUM'] == random_client]
features_client_num = client_row[selected_features].values.reshape(1, -1)

# Scale the features using the loaded scaler
features_client_num_scaled = scaler.transform(features_client_num)

# Get churn probability
churn_prob = svm_model.predict_proba(features_client_num_scaled)[:, 1]

print(f'Churn rate for CLIENTNUM {random_client}: {churn_prob[0]*100:.2f}%')
print(f'Not Churn rate for CLIENTNUM {random_client}: {(1 - churn_prob[0])*100:.2f}%')

!pip install gradio

!ls *.pkl

preprocessed_df.shape

preprocessed_df.head()

"""#Saving the models"""

# Save models
joblib.dump(logreg, "logreg_model.joblib")
model_naive_bayes = joblib.load('naive_bayes_model.joblib')
joblib.dump(svm_model, "svm_model.joblib")

import gradio as gr
import pandas as pd
import joblib
import plotly.graph_objects as go


# Load required files
preprocessed_df = pd.read_csv("preprocessed_df.csv")
selected_features = joblib.load("selected_features.pkl")
logreg_model = joblib.load("logreg_model.joblib")
naive_bayes_model = joblib.load("naive_bayes_model.joblib")
svm_model = joblib.load("svm_model.joblib")
scaler = joblib.load("scaler.joblib")

# Map CLIENTNUM to preprocessed data index
clientnum_to_index = {num: idx for idx, num in enumerate(preprocessed_df['CLIENTNUM'])}

# Mappings for categorical columns
gender_mapping = {0: "Female", 1: "Male"}
education_level_mapping = {0: "College", 1: "Doctorate", 2: "Graduate", 3: "High School",
                         4: "Post-Graduate", 5: "Uneducated", 6: "Unknown"}
marital_status_mapping = {0: "Divorced", 1: "Married", 2: "Single", 3: "Unknown"}
income_category_mapping = {0: "$120K +", 1: "$40K - $60K", 2: "$60K - $80K", 3: "$80K - $120K",
                         4: "Less than $40K", 5: "Unknown"}
card_category_mapping = {0: "Blue", 1: "Gold", 2: "Platinum", 3: "Silver"}

def predict_churn_with_visuals(clientnum):
    if int(clientnum) not in clientnum_to_index:
        return None, None

    customer_index = clientnum_to_index[int(clientnum)]
    customer_features = preprocessed_df.iloc[customer_index][selected_features].values.reshape(1, -1)
    customer_features_scaled = scaler.transform(customer_features)

    models = {
        "Logistic Regression": logreg_model,
        "Naive Bayes": naive_bayes_model,
        "SVM": svm_model
    }

    churn_probs = []
    for model in models.values():
        pred = model.predict_proba(customer_features_scaled)
        churn_probs.append(pred[0][1] * 100)

    avg_churn_prob = sum(churn_probs) / len(churn_probs)

    # Gauge chart
    gauge_chart = go.Figure(go.Indicator(
        mode="gauge+number",
        value=avg_churn_prob,
        title={"text": "Churn Probability", "font": {"color": "white"}},
        gauge={
            "axis": {"range": [0, 100], "tickwidth": 2, "tickcolor": "white"},
            "bar": {"color": "rgba(200, 0, 0, 0.8)" if avg_churn_prob > 50 else "rgba(0, 128, 0, 0.8)"},
            "steps": [
                {"range": [0, 30], "color": "rgba(144, 238, 144, 0.5)"},
                {"range": [30, 70], "color": "rgba(255, 255, 102, 0.5)"},
                {"range": [70, 100], "color": "rgba(255, 99, 71, 0.5)"}
            ],
            "borderwidth": 2,
            "bordercolor": "white"
        }
    ))
    gauge_chart.update_layout(
        paper_bgcolor="rgba(0,0,0,0)",
        font_color="white"
    )

    # Bar chart
    bar_chart = go.Figure(go.Bar(
        x=churn_probs,
        y=list(models.keys()),
        orientation="h",
        marker_color=["#1f77b4", "#ff7f0e", "#2ca02c"],
        text=[f"{val:.2f}%" for val in churn_probs],
        textposition="outside"
    ))
    bar_chart.update_layout(
        title="Churn Probability by Model",
        xaxis=dict(
            title="Probability (%)",
            showgrid=False,
            zeroline=False,
            linecolor="white",
            range=[0, 110],
        ),
        yaxis=dict(
            title="Models",
            showgrid=False,
            zeroline=False,
            linecolor="white",
        ),
        plot_bgcolor="rgba(0,0,0,0)",
        paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="white"),
        margin=dict(r=20, t=40, b=40, l=20),
    )

    return gauge_chart, bar_chart

def get_customer_info(clientnum):
    if int(clientnum) not in clientnum_to_index:
        return None

    customer_index = clientnum_to_index[int(clientnum)]
    customer_data = preprocessed_df.iloc[customer_index]

    return {
        "Customer_Age": int(customer_data["Customer_Age"]),
        "Gender": gender_mapping.get(customer_data["Gender"], "Unknown"),
        "Dependent_count": int(customer_data["Dependent_count"]),
        "Education_Level": education_level_mapping.get(customer_data["Education_Level"], "Unknown"),
        "Marital_Status": marital_status_mapping.get(customer_data["Marital_Status"], "Unknown"),
        "Income_Category": income_category_mapping.get(customer_data["Income_Category"], "Unknown"),
        "Card_Category": card_category_mapping.get(customer_data["Card_Category"], "Unknown"),
        "Months_on_book": int(customer_data["Months_on_book"]),
        "Total_Relationship_Count": int(customer_data["Total_Relationship_Count"]),
        "Months_Inactive_12_mon": int(customer_data["Months_Inactive_12_mon"]),
        "Contacts_Count_12_mon": int(customer_data["Contacts_Count_12_mon"]),
        "Credit_Limit": float(customer_data["Credit_Limit"]),
        "Total_Revolving_Bal": float(customer_data["Total_Revolving_Bal"]),
        "Avg_Open_To_Buy": float(customer_data["Avg_Open_To_Buy"]),
        "Total_Trans_Amt": float(customer_data["Total_Trans_Amt"]),
        "Total_Trans_Ct": int(customer_data["Total_Trans_Ct"]),
        "Avg_Utilization_Ratio": float(customer_data["Avg_Utilization_Ratio"])
    }

def display_customer_info_and_predict(clientnum):
    info = get_customer_info(clientnum)
    if info is None:
        return [None] * 19  # Return None for all outputs

    gauge_chart, bar_chart = predict_churn_with_visuals(clientnum)

    return [
        info["Customer_Age"],
        info["Gender"],
        info["Dependent_count"],
        info["Education_Level"],
        info["Marital_Status"],
        info["Income_Category"],
        info["Card_Category"],
        info["Months_on_book"],
        info["Total_Relationship_Count"],
        info["Months_Inactive_12_mon"],
        info["Contacts_Count_12_mon"],
        info["Credit_Limit"],
        info["Total_Revolving_Bal"],
        info["Avg_Open_To_Buy"],
        info["Total_Trans_Amt"],
        info["Total_Trans_Ct"],
        info["Avg_Utilization_Ratio"],
        gauge_chart,
        bar_chart
    ]

# Gradio Interface
# theme=gr.themes.Base(primary_hue="slate")
js_func = """
function refresh() {
    const url = new URL(window.location);

    if (url.searchParams.get('__theme') !== 'dark') {
        url.searchParams.set('__theme', 'dark');
        window.location.href = url.href;
    }
}
"""
with gr.Blocks(js = js_func) as demo:
    gr.Markdown("# Customer Churn Prediction Dashboard")
    gr.Markdown("Select a customer by their Customer ID and click 'Update Prediction' to see the results.")

    with gr.Row():
        clientnum_dropdown = gr.Dropdown(
            label="Customer ID",
            choices=preprocessed_df['CLIENTNUM'].astype(str).tolist(),
            value=preprocessed_df['CLIENTNUM'].astype(str).tolist()[0]
        )
        update_button = gr.Button("Update Prediction")

    with gr.Row():
        # Customer Information
        with gr.Column():
            age = gr.Number(label="Customer Age", interactive=False)
            gender = gr.Text(label="Gender", interactive=False)
            dependent_count = gr.Number(label="Number of Dependents", interactive=False)
            education_level = gr.Text(label="Education Level", interactive=False)
            marital_status = gr.Text(label="Marital Status", interactive=False)

        with gr.Column():
            income_category = gr.Text(label="Annual Income Range", interactive=False)
            card_category = gr.Text(label="Credit Card Type", interactive=False)
            months_on_book = gr.Number(label="Duration as Customer (Months)", interactive=False)
            total_relationships = gr.Number(label="Number of Bank Products", interactive=False)
            months_inactive = gr.Number(label="Months of Inactivity (Past Year)", interactive=False)

        with gr.Column():
            contacts_count = gr.Number(label="Customer Service Contacts (Past Year)", interactive=False)
            credit_limit = gr.Number(label="Credit Card Limit ($)", interactive=False)
            total_revolving_bal = gr.Number(label="Current Balance ($)", interactive=False)
            avg_open_to_buy = gr.Number(label="Available Credit ($)", interactive=False)
            total_trans_amt = gr.Number(label="Total Transaction Amount (Past Year) ($)", interactive=False)
            total_trans_ct = gr.Number(label="Number of Transactions (Past Year)", interactive=False)
            avg_utilization_ratio = gr.Number(label="Credit Utilization Rate (%)", interactive=False)

    with gr.Row():
        churn_gauge = gr.Plot(label="Average Churn Probability")
        churn_bar = gr.Plot(label="Churn Probability by Model")

    update_button.click(
        display_customer_info_and_predict,
        inputs=[clientnum_dropdown],
        outputs=[
            age, gender, dependent_count, education_level, marital_status,
            income_category, card_category, months_on_book, total_relationships,
            months_inactive, contacts_count, credit_limit, total_revolving_bal,
            avg_open_to_buy, total_trans_amt, total_trans_ct, avg_utilization_ratio,
            churn_gauge, churn_bar
        ]
    )

if __name__ == "__main__":
    demo.launch()

from google.colab import files

# Save the notebook content as a Python file
with open('app.py', 'w') as f:
    f.write('''import gradio as gr
import pandas as pd
import joblib
import plotly.graph_objects as go

# Load required files
preprocessed_df = pd.read_csv("preprocessed_df.csv")
selected_features = joblib.load("selected_features.pkl")
logreg_model = joblib.load("logreg_model.joblib")
naive_bayes_model = joblib.load("naive_bayes_model.joblib")
svm_model = joblib.load("svm_model.joblib")
scaler = joblib.load("scaler.joblib")

# Map CLIENTNUM to preprocessed data index
clientnum_to_index = {num: idx for idx, num in enumerate(preprocessed_df['CLIENTNUM'])}

# Mappings for categorical columns
gender_mapping = {0: "Female", 1: "Male"}
education_level_mapping = {0: "College", 1: "Doctorate", 2: "Graduate", 3: "High School",
                         4: "Post-Graduate", 5: "Uneducated", 6: "Unknown"}
marital_status_mapping = {0: "Divorced", 1: "Married", 2: "Single", 3: "Unknown"}
income_category_mapping = {0: "$120K +", 1: "$40K - $60K", 2: "$60K - $80K", 3: "$80K - $120K",
                         4: "Less than $40K", 5: "Unknown"}
card_category_mapping = {0: "Blue", 1: "Gold", 2: "Platinum", 3: "Silver"}

def predict_churn_with_visuals(clientnum):
    if int(clientnum) not in clientnum_to_index:
        return None, None

    customer_index = clientnum_to_index[int(clientnum)]
    customer_features = preprocessed_df.iloc[customer_index][selected_features].values.reshape(1, -1)
    customer_features_scaled = scaler.transform(customer_features)

    models = {
        "Logistic Regression": logreg_model,
        "Naive Bayes": naive_bayes_model,
        "SVM": svm_model
    }

    churn_probs = []
    for model in models.values():
        pred = model.predict_proba(customer_features_scaled)
        churn_probs.append(pred[0][1] * 100)

    avg_churn_prob = sum(churn_probs) / len(churn_probs)

    # Gauge chart
    gauge_chart = go.Figure(go.Indicator(
        mode="gauge+number",
        value=avg_churn_prob,
        title={"text": "Churn Probability", "font": {"color": "white"}},
        gauge={
            "axis": {"range": [0, 100], "tickwidth": 2, "tickcolor": "white"},
            "bar": {"color": "rgba(200, 0, 0, 0.8)" if avg_churn_prob > 50 else "rgba(0, 128, 0, 0.8)"},
            "steps": [
                {"range": [0, 30], "color": "rgba(144, 238, 144, 0.5)"},
                {"range": [30, 70], "color": "rgba(255, 255, 102, 0.5)"},
                {"range": [70, 100], "color": "rgba(255, 99, 71, 0.5)"}
            ],
            "borderwidth": 2,
            "bordercolor": "white"
        }
    ))
    gauge_chart.update_layout(
        paper_bgcolor="rgba(0,0,0,0)",
        font_color="white"
    )

    # Bar chart
    bar_chart = go.Figure(go.Bar(
        x=churn_probs,
        y=list(models.keys()),
        orientation="h",
        marker_color=["#1f77b4", "#ff7f0e", "#2ca02c"],
        text=[f"{val:.2f}%" for val in churn_probs],
        textposition="outside"
    ))
    bar_chart.update_layout(
        title="Churn Probability by Model",
        xaxis=dict(
            title="Probability (%)",
            showgrid=False,
            zeroline=False,
            linecolor="white",
            range=[0, 110],
        ),
        yaxis=dict(
            title="Models",
            showgrid=False,
            zeroline=False,
            linecolor="white",
        ),
        plot_bgcolor="rgba(0,0,0,0)",
        paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="white"),
        margin=dict(r=20, t=40, b=40, l=20),
    )

    return gauge_chart, bar_chart

def get_customer_info(clientnum):
    if int(clientnum) not in clientnum_to_index:
        return None

    customer_index = clientnum_to_index[int(clientnum)]
    customer_data = preprocessed_df.iloc[customer_index]

    return {
        "Customer_Age": int(customer_data["Customer_Age"]),
        "Gender": gender_mapping.get(customer_data["Gender"], "Unknown"),
        "Dependent_count": int(customer_data["Dependent_count"]),
        "Education_Level": education_level_mapping.get(customer_data["Education_Level"], "Unknown"),
        "Marital_Status": marital_status_mapping.get(customer_data["Marital_Status"], "Unknown"),
        "Income_Category": income_category_mapping.get(customer_data["Income_Category"], "Unknown"),
        "Card_Category": card_category_mapping.get(customer_data["Card_Category"], "Unknown"),
        "Months_on_book": int(customer_data["Months_on_book"]),
        "Total_Relationship_Count": int(customer_data["Total_Relationship_Count"]),
        "Months_Inactive_12_mon": int(customer_data["Months_Inactive_12_mon"]),
        "Contacts_Count_12_mon": int(customer_data["Contacts_Count_12_mon"]),
        "Credit_Limit": float(customer_data["Credit_Limit"]),
        "Total_Revolving_Bal": float(customer_data["Total_Revolving_Bal"]),
        "Avg_Open_To_Buy": float(customer_data["Avg_Open_To_Buy"]),
        "Total_Trans_Amt": float(customer_data["Total_Trans_Amt"]),
        "Total_Trans_Ct": int(customer_data["Total_Trans_Ct"]),
        "Avg_Utilization_Ratio": float(customer_data["Avg_Utilization_Ratio"])
    }

def display_customer_info_and_predict(clientnum):
    info = get_customer_info(clientnum)
    if info is None:
        return [None] * 19  # Return None for all outputs

    gauge_chart, bar_chart = predict_churn_with_visuals(clientnum)

    return [
        info["Customer_Age"],
        info["Gender"],
        info["Dependent_count"],
        info["Education_Level"],
        info["Marital_Status"],
        info["Income_Category"],
        info["Card_Category"],
        info["Months_on_book"],
        info["Total_Relationship_Count"],
        info["Months_Inactive_12_mon"],
        info["Contacts_Count_12_mon"],
        info["Credit_Limit"],
        info["Total_Revolving_Bal"],
        info["Avg_Open_To_Buy"],
        info["Total_Trans_Amt"],
        info["Total_Trans_Ct"],
        info["Avg_Utilization_Ratio"],
        gauge_chart,
        bar_chart
    ]

# Gradio Interface
js_func = """
function refresh() {
    const url = new URL(window.location);

    if (url.searchParams.get('__theme') !== 'dark') {
        url.searchParams.set('__theme', 'dark');
        window.location.href = url.href;
    }
}
"""
with gr.Blocks(js = js_func) as demo:
    gr.Markdown("# Customer Churn Prediction Dashboard")
    gr.Markdown("Select a customer by their Customer ID and click 'Update Prediction' to see the results.")

    with gr.Row():
        clientnum_dropdown = gr.Dropdown(
            label="Customer ID",
            choices=preprocessed_df['CLIENTNUM'].astype(str).tolist(),
            value=preprocessed_df['CLIENTNUM'].astype(str).tolist()[0]
        )
        update_button = gr.Button("Update Prediction")

    with gr.Row():
        # Customer Information
        with gr.Column():
            age = gr.Number(label="Customer Age", interactive=False)
            gender = gr.Text(label="Gender", interactive=False)
            dependent_count = gr.Number(label="Number of Dependents", interactive=False)
            education_level = gr.Text(label="Education Level", interactive=False)
            marital_status = gr.Text(label="Marital Status", interactive=False)

        with gr.Column():
            income_category = gr.Text(label="Annual Income Range", interactive=False)
            card_category = gr.Text(label="Credit Card Type", interactive=False)
            months_on_book = gr.Number(label="Duration as Customer (Months)", interactive=False)
            total_relationships = gr.Number(label="Number of Bank Products", interactive=False)
            months_inactive = gr.Number(label="Months of Inactivity (Past Year)", interactive=False)

        with gr.Column():
            contacts_count = gr.Number(label="Customer Service Contacts (Past Year)", interactive=False)
            credit_limit = gr.Number(label="Credit Card Limit ($)", interactive=False)
            total_revolving_bal = gr.Number(label="Current Balance ($)", interactive=False)
            avg_open_to_buy = gr.Number(label="Available Credit ($)", interactive=False)
            total_trans_amt = gr.Number(label="Total Transaction Amount (Past Year) ($)", interactive=False)
            total_trans_ct = gr.Number(label="Number of Transactions (Past Year)", interactive=False)
            avg_utilization_ratio = gr.Number(label="Credit Utilization Rate (%)", interactive=False)

    with gr.Row():
        churn_gauge = gr.Plot(label="Average Churn Probability")
        churn_bar = gr.Plot(label="Churn Probability by Model")

    update_button.click(
        display_customer_info_and_predict,
        inputs=[clientnum_dropdown],
        outputs=[
            age, gender, dependent_count, education_level, marital_status,
            income_category, card_category, months_on_book, total_relationships,
            months_inactive, contacts_count, credit_limit, total_revolving_bal,
            avg_open_to_buy, total_trans_amt, total_trans_ct, avg_utilization_ratio,
            churn_gauge, churn_bar
        ]
    )

if __name__ == "__main__":
    demo.launch()
''')

# # Download the file
# files.download('app.py')

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Clone the repository
!git clone https://huggingface.co/spaces/Maryamarshad015/Customer_Churn_Dashboard

# Step 2: Copy your files into the repository directory
!cp app.py preprocessed_df.csv logreg_model.joblib naive_bayes_model.joblib scaler.joblib svm_model.joblib selected_features.pkl BankChurners.csv /content/Customer_Churn_Dashboard/app.py

# Step 3: Navigate to the repository directory
# %cd Customer_Churn_Dashboard

# Step 4: Add and commit files
!git add .
!git commit -m "Add app.py and other files"

# Step 5: Push to Hugging Face (replace <your-token> with your actual token)
!git push https://Maryamarshad015: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@huggingface.co/spaces/Maryamarshad015/Customer_Churn_Dashboard.git main

!git add svm_model.joblib selected_features.pkl
!git commit -m "Add app.py and other files"

!git config --user.email "Maryamarshad015@gmail.com"
  !git config --user.name "Maryamarshad015"

