# -*- coding: utf-8 -*-
"""BankChurners_Churn_Prediction_Dashboard_Finale.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vuJKaE11WmpXVgo0-Rc03lLeOvSiaFar
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

dataset_check = pd.read_csv('BankChurners.csv')

dataset_check.head()

dataset_check.info()

dataset_check.isnull().sum()

dataset_check.describe()

dataset_check=dataset_check.drop(dataset_check.iloc[:,21:],axis=1)

dataset_check.head()

dataset_check['Attrition_Flag'].unique()

dataset_check.shape

dataset_check.columns

df_all=dataset_check.drop_duplicates()

df_all.drop_duplicates().shape
#it means the dataset had no duplicates

#if dataset had duplicates we could remove it like this
#df_all = df_all.drop_duplicates(subset=None , keep='first')
#but we had no duplicates so no need to try this method at any column

import matplotlib.pyplot as plt
plt.figure(figsize=(5,3))



ax=sns.histplot(data=df_all,
               x='Customer_Age',
               bins=range(df_all['Customer_Age'].min(),df_all['Customer_Age'].max())
               )

for p in ax.patches:
    height=p.get_height()
    ax.annotate(
        f'{height}',
        (p.get_x()+p.get_width()/2,height),
        ha='center',va='bottom',
        xytext=(0,2),
        textcoords='offset points',
        fontsize=7,
        rotation=90,

    )

df_all['Credit_Limit'].dtype

#create automatic one function for histograms for labelling median dotted line with linestyle=":" also in the histogram
def histoplotting(column_name,hue_=None):

    if df_all[column_name].dtype == 'float64':
        df_all[column_name]=df_all[column_name].astype('int64')

    plt.figure(figsize=(6,3))

    maximum = df_all[column_name].max()
    minimum = df_all[column_name].min()

    median = df_all[column_name].median()

    if hue_:
        ax=sns.histplot(x=df_all[column_name],
                        bins=range(minimum,maximum+1),
                        hue=hue_,
                        multiple='dodge'
                       )
    else:
         ax=sns.histplot(x=df_all[column_name],
                        bins=range(minimum,maximum+1)
                        )


    #labelling the bars in the histogram
    #running a for loop for each bar

    for p in ax.patches:
        height = p.get_height()
        ax.annotate(
            f'{height}',
            (p.get_x()+p.get_width()/2,p.get_height()),
            xytext=(0,4),
            textcoords='offset points',
            va='bottom',ha='center',
            rotation=90,
            fontsize=7
        )
    #outside the for loop as median is singular
    #represent the axv line using dotted red color line for the representation of median in the particular column

    plt.axvline(x=median, linestyle=":",color='red',linewidth=2)
    #for labelling median axvline using plotlib
    plt.text(median, plt.ylim()[0]-0*plt.ylim()[1],f'Median={median}',color='red',ha='center',va='top')
    #plt.ylim() return tuple(ymin,ymax) in order to  get just upper limit of y axis we use plt.ylim()[1]


    plt.show()

histoplotting('Customer_Age')

df_all.head()

df_all['Avg_Utilization_Ratio'] = df_all['Avg_Utilization_Ratio'].round(1)
df_all.head()

bins= pd.cut(df_all['Customer_Age'],bins=4)
labels_tuples = bins.cat.categories
labels_list = list(f'{int(left)}-{int(right)}'for left,right in zip(labels_tuples.left,labels_tuples.right))


#intorduced a new column in the dataframe df_all
df_all['Customer_Age_Seg'] = pd.cut(df_all['Customer_Age'],bins=4,labels=labels_list)
df_all['Customer_Age_Seg'].head()

df_all.head()

df_income_age = df_all.groupby(['Income_Category','Customer_Age_Seg'])['Customer_Age_Seg'].value_counts().reset_index()

barplotting(df_income_age,'Income_Category','count','Customer_Age_Seg')

plt.figure(figsize=(7,5))


colors = ['blue','green','red','yellow']
sns.histplot(data=df_income_age,
           x='Income_Category',
           hue='Customer_Age_Seg',
           weights='count',
               shrink=0.8,
           palette=colors,
           multiple='stack')

plt.show()

barplotting(df_all,'Marital_Status','Avg_Utilization_Ratio')

#Segment customers by income level and analyze spending patterns or churn rates.
df_churn_income = df_all.groupby('Income_Category')['Attrition_Flag'].value_counts().reset_index()
df_churn_income

barplotting(df_churn_income,'Income_Category','count','Attrition_Flag')

#Exploring the transaction frequency of each customer and based on dependent_count as a hue
order = [0,1,2,3,4,5]
df_all['Dependent_count'] = pd.Categorical(df_all['Dependent_count'],categories=order,ordered=True)

plt.figure(figsize = (10,5))


minimum = df_all['Total_Trans_Ct'].min()
maximum = df_all['Total_Trans_Ct'].max()

median = df_all['Total_Trans_Ct'].median()
ax=sns.histplot(data = df_all,
            x='Total_Trans_Ct',
            bins=range(minimum,maximum+1,10),
            hue='Dependent_count',
            multiple='dodge'
               )

#outside the for loop as median is singular
#represent the axv line using dotted red color line for the representation of median in the particular column

plt.axvline(x=median, linestyle=":",color='red',linewidth=2)
#for labelling median axvline using plotlib
plt.text(median,plt.ylim()[1]*0.9,f'Median={median}',color='red',ha='center',va='center')
#plt.ylim() return tuple(ymin,ymax) in order to  get just upper limit of y axis we use plt.ylim()[1]
plt.xlabel(range(minimum,maximum+1,10))


plt.show()

#creating seperate column for transaction count categories and then creating a dataframe for dependents vs trans ct.
#creating a stack histplot for this purpose

bins_edges = pd.cut(df_all['Total_Trans_Ct'],bins=4).cat.categories
#picking out list of categories
labels = list(f'{int(left)}-{int(right)}'for left,right in zip(bins_edges.left,bins_edges.right))

df_all['Trans_Ct_Seg'] = pd.cut(df_all['Total_Trans_Ct'],bins=4,labels=labels)

#creating a datframe to calculate the total recordings in the dataset for the transaction counts categories
#taking depemdent count as a hue

df_trans_dep = df_all.groupby(['Trans_Ct_Seg','Dependent_count'])['Dependent_count'].value_counts().reset_index()
df_trans_dep.head()

#create a histplot with count as a weights and dependent count as a hue
plt.figure(figsize=(5,5))

sns.histplot(data=df_trans_dep,
            x='Trans_Ct_Seg',
            hue='Dependent_count',
            weights='count',
            shrink=0.7,
            multiple='stack')

plt.title('Transactions VS Dependents of a Family')
plt.show()

order=[ 'Unknown','Less than $40K','$120K +', '$40K - $60K', '$60K - $80K', '$80K - $120K']

df_all['Income_Category'] = pd.Categorical(df_all['Income_Category'], categories=order)

"""Average Credit Limit: Analyze the average credit limit by customer segment (e.g., by income, age group, or gender)."""

#Grouping the avg credit limits according to income categories but remember to analyze AVG
#Analyzing according to hue married/single
df_income_vs_credit = df_all[['Income_Category','Credit_Limit','Marital_Status']].groupby(['Income_Category','Marital_Status'])['Credit_Limit'].mean().reset_index()
df_income_vs_credit['Credit_Limit'] = df_income_vs_credit['Credit_Limit'].round(1)

df_income_vs_credit

#Now showing this above dataframe through bar plot giving marital status as a hue
barplotting(df_income_vs_credit,'Income_Category','Credit_Limit','Marital_Status')

#and creating without hue
barplotting(df_income_vs_credit,'Income_Category','Credit_Limit')

#Observe the male/female category in each eduaction level criterion
#for this purpose creating a seperate dataframe so that no complexity is to be faced

df_edu_gender = df_all.groupby('Education_Level')['Gender'].value_counts().reset_index()

barplotting(df_edu_gender,'Education_Level','count','Gender')

#lets find out males or females have spent more months on book
#obviously the population of females is more but what if unexpectedtly males might have spent more months on book

#we will be just calculating the average months on book for males and females
df_avg_months = df_all[['Gender','Months_on_book']].groupby('Gender').agg({'Months_on_book':'mean'}).reset_index()
#df_avg_months

#displaying it in the form of pie chart
plt.figure(figsize=(3,3))
labels=df_avg_months['Gender'].unique()
plt.pie(df_avg_months['Months_on_book'],labels=labels,autopct='%1.1f%%',startangle=145,colors=['pink','skyblue'])
plt.plot()

"""Despite the fact that there were more females approx 5% more than males but still both spent almost equal months on bank."""

df_all['Gender'] = pd.Categorical(df_all['Gender'])

df_gen_count = df_all['Gender'].value_counts().reset_index()
plt.figure(figsize=(3,3))
labels=df_gen_count['Gender'].unique()


plt.pie(df_gen_count['count'],labels=labels,autopct="%1.1f%%",startangle=45,colors=['pink','skyblue'])
plt.plot()

#seeing the categories of education level how many people belong to which category and having Card_Category as a hue
df_all['Education_Level'] = pd.Categorical(df_all['Education_Level'])
df_edu = df_all[['Education_Level','Card_Category']].groupby(['Education_Level','Card_Category']).value_counts().reset_index()
df_edu

#creating a histogram for having card as a hue

plt.figure(figsize=(10,3))

sns.barplot(data=df_edu,
            x='Education_Level',
            y='count',
            hue='Card_Category')

#creating a dataframe for comparing card holders and their attrition flag status regarding their gender
df_att_card = df_all[['Card_Category','Attrition_Flag','Gender']].groupby(['Card_Category','Attrition_Flag','Gender']).value_counts().reset_index()
df_att_card

#creating histogram for comparing card holders and attrition flag but having gender as a hue

plt.figure(figsize=(7,5))

sns.barplot(data=df_att_card,
           x='Card_Category',
           y='count',
           hue='Attrition_Flag',
           palette='pastel')
plt.plot()

#creating a dataframe for male/female holding which card categories
#creating a pie chart for showing the percentages of male female different card category holders

df_all['Card_Category'].unique()

#converting card categories into categorical columns
order = ['Blue','Platinum','Silver','Gold']
df_all['Card_Category'] = pd.Categorical(df_all['Card_Category'],
                                         categories=order,
                                         ordered=True)

df_card =  df_all[['Gender','Card_Category']].groupby(['Gender','Card_Category']).value_counts().reset_index()
df_card

barplotting(df_card,'Card_Category','count','Gender')

#introducing a new quartile column
df_all['Revolving_Bal_Quartile'],q_edges= pd.qcut(df_all['Total_Revolving_Bal'],
                q=4,
                labels=['Q1','Q2','Q3','Q4'],
                retbins=True)
df_all.head()

df_income = df_all[['Education_Level','Income_Category']].groupby(['Education_Level','Income_Category']).value_counts().reset_index()

df_income

barplotting(df_income,'Education_Level','count','Income_Category')

#grouping according to balance quarter ranges and according to hue of education/salary/Attrition Flag Status
df_bal = df_all[['Revolving_Bal_Quartile','Attrition_Flag']].groupby(['Revolving_Bal_Quartile','Attrition_Flag']).value_counts().reset_index()
df_bal

print(q_edges)
#q_edges has my revolving balance categories stored which i will be plotting on axis as plt.x_label

barplotting(df_bal,'Revolving_Bal_Quartile','count','Attrition_Flag')
#plt.xlabel(q_edges)

histoplotting('Months_on_book')

df_all['Months_Inactive_12_mon'].unique()

#creating a mini dataset of counting how many customers were inactive for how many months in the past 12 months inactivation

mini_df = df_all[['Months_Inactive_12_mon','Gender']].value_counts()
mini_df = mini_df.reset_index(name='count')
mini_df = mini_df.sort_values(by='Months_Inactive_12_mon',ascending=True)

mini_df

def barplotting(df,col1,col2,hue_=None):
    plt.figure(figsize=(10,4))

    if hue_:
        ax=sns.barplot(data=df,
                      x=col1,
                      y=col2,
                      hue=hue_,
                      palette='viridis')
    if not hue_:
        ax=sns.barplot(data=df,
                      x=col1,
                      y=col2,
                      #hue=hue_,
                      palette='viridis')

    #labelling purposes
    for p in ax.patches:
        height = p.get_height()
        ax.annotate(
            f'{height:.0f}',
            (p.get_x()+p.get_width()/2,p.get_height()),
            xytext=(0,4),
            textcoords='offset points',
            va='bottom',ha='center',
            rotation=90,
            fontsize=7
        )

    plt.show()

barplotting(mini_df,'Months_Inactive_12_mon','count','Gender')

plt.figure(figsize=(6,3))

ax = sns.barplot(
           x='Months_Inactive_12_mon',
           y= 'count',
           hue='Gender',
          data = mini_df,
    palette = 'viridis'
)


for p in ax.patches:
    height = p.get_height()
    ax.annotate(
        f'{height}',
        (p.get_x()+p.get_width()/2,p.get_height()),
        xytext=(0,4),
        textcoords='offset points',
        va='bottom',ha='center',
        rotation=90,
        fontsize=7
    )

plt.plot()

histoplotting('Months_Inactive_12_mon')

df_all.shape

# # Map 'Attrited Customer' to 1 and 'Existing Customer' to 0
# df_all['Attrition_Flag'] = df_all['Attrition_Flag'].map({
#     'Attrited Customer': 0,
#     'Existing Customer': 1
# })

# # Verify the conversion
# df_all['Attrition_Flag'].unique()

columns_to_drop = df_all.iloc[:,21:]
df_all.drop(columns_to_drop,axis=1,inplace=True)

df_all.columns

# Columns to encode
col = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']

df_all[col] = df_all[col].astype('category')

#apply cat.codes method to make binary + multiple encoding columns

df_all[col] = df_all[col].apply(lambda x: x.cat.codes)
df_all.head()

# Assuming `dataset_check` is the DataFrame with the original categorical values

# Columns to encode (as per your previous code)
col = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']

# Dictionary to store mappings of category labels to codes from `dataset_check`
categorical_mappings = {}

# Retrieve category labels from `dataset_check` before encoding
for column in col:
    # Ensure the column in `dataset_check` is of category dtype
    dataset_check[column] = dataset_check[column].astype('category')

    # Store the category labels and their corresponding numeric codes
    categorical_mappings[column] = {code: category for code, category in enumerate(dataset_check[column].cat.categories)}

# Print the mappings (numeric codes -> category labels)
for column, mapping in categorical_mappings.items():
    print(f"{column} Mapping (Code -> Category):")
    for code, category in mapping.items():
        print(f"Code {code} -> {category}")
    print()

# Step 1: Ensure columns are converted to 'category' dtype
col = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']

# Dictionary to store mappings of category labels to codes
categorical_mappings = {}

# Convert columns to 'category' dtype and store the mappings of labels to codes
for column in col:
    # Convert to category dtype
    df_all[column] = df_all[column].astype('category')

    # Save the categories and their corresponding integer codes (before applying .cat.codes)
    categorical_mappings[column] = {category: code for code, category in enumerate(df_all[column].cat.categories)}

# Step 2: Apply .cat.codes to the columns (encode them)
df_all[col] = df_all[col].apply(lambda x: x.cat.codes)

# Step 3: Print the mappings (category labels -> corresponding codes)
for column, mapping in categorical_mappings.items():
    print(f"{column} Mapping:")
    print(mapping)
    print()

# Step 4: Optionally, display the transformed DataFrame to verify the encoding
print(df_all.head())

numerical = df_all.select_dtypes(include=['number'])
categorical = df_all['Attrition_Flag']
df_all_features = pd.concat([numerical,categorical], axis=1)
df_all_features.columns

df_all_features.shape

columns_drop  = df_all_features[['CLIENTNUM','Total_Amt_Chng_Q4_Q1','Total_Ct_Chng_Q4_Q1']]
features_df = df_all_features.drop(columns_drop, axis=1)
features_df.head()

features_df.shape

#visualizing outliers in the Customer Age
plt.figure(figsize=(5,3))
sns.boxplot(features_df, x= 'Customer_Age')

"""Deciding to keep the outliers as the customers with old age might be meaningful for predicting churn rate."""

plt.figure(figsize=(5,3))
sns.boxplot(features_df, x= 'Total_Trans_Ct')

# #creating a pair plot to observe relations between different columns
# import matplotlib.pyplot as plt
# import seaborn as sns
# #hue can be set to see the churn and non-churn in two different colours (Exisitng / Attrited Customer)
# plt.figure(figsize=(5,5))
# sns.pairplot(features_df, hue = 'Attrition_Flag')
# plt.show()

#converting Attrited_Flag column into numerical
# Map 'Attrited Customer' to 1 and 'Existing Customer' to 0
features_df['Attrition_Flag'] = features_df['Attrition_Flag'].map({
    'Attrited Customer': 0,
    'Existing Customer': 1
})

# Verify the conversion
features_df['Attrition_Flag'].unique()

"""The two classes are pretty imbalanced non-churners are a minority class.
Even after applying class_weight='balanced' the LogisticRegression is not producing good results.
Hence applying:
#SMOTE TECHNIQUE:
OverSampling the minority class.

Furthermore this sampling technique is applied only on training data.

"""

df = features_df.groupby('Attrition_Flag').size()
print(df)

"""Finding out 10 best features out of 16 to include in our classical ML Models using:


1.RFE (RECUSRSIVE FEATURE ELIMINATION)

2.LassoCV

#RECURSIVE FEATURE ELIMINATION
"""

from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
import joblib

X = features_df.drop('Attrition_Flag', axis=1)
y = features_df['Attrition_Flag']


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
rfe = RFE(estimator=LogisticRegression(), n_features_to_select=17, step=1) #remove 1 feature in each step
rfe.fit(X_scaled, y)

# Get the selected features
selected_features = X.columns[rfe.support_]
selected_features.shape

joblib.dump(selected_features, "selected_features.pkl")

"""for improved and better performance over the class 0 : non-churners.
Include the more amount of columns/features and then try running your model to see the evaluation results.

Like your task is to encode the 'marital status', 'card category', 'income category','education category' such columns.

Make new columns in the dataframe for each of the above mentioned columns as like 'marital status encoded' and include those new columns in the lasso CV for selecting features and then run you model and see the performance.
"""

# for improved and better performance over the class 0 : non-churners.
# Include the more amount of columns/features and then try running your model to see the evaluation results.

# Like your task is to encode the 'marital status', 'card category', 'income category','education category' such columns.

# Make new columns in the dataframe for each of the above mentioned columns as like 'marital status encoded'
# and include those new columns in the lasso CV for selecting features and then run you model and see the performance.

"""simply map the binary categorical columns .
no need to create new columns in the dataframe for such columns.

just like you mapped the' attrited flag ' column .

and encode other categorical columns having more than one categories.
"""

# #using lassoCV for selecting features
# from sklearn.linear_model import LassoCV
# from sklearn.preprocessing import StandardScaler



# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# lasso = LassoCV(cv=17, random_state=42)
# lasso.fit(X_scaled, y)

# selected_features = X.columns[lasso.coef_ != 0]
# selected_features.shape

"""#LOGISTIC REGRESSION

Predicting Churn Rate
"""

pip install imbalanced-learn

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

#splitting data
X_selected = selected_features
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 42)

#applying SMOTE
X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)

#initializing the LogisticRegression Model

logreg = LogisticRegression(class_weight = 'balanced', C=1.0, random_state = 42)

#fitting the model on SMOTE resampled data

logreg.fit(X_train_resampled,y_train_resampled)

#predecting
y_pred = logreg.predict(X_test)
y_pred_proba = logreg.predict_proba(X_test)

"""#CLASSIFICATION REPORT"""

print(classification_report(y_test,y_pred))

from sklearn.metrics import confusion_matrix
conf = confusion_matrix(y_test,y_pred)

#plotting heatmap
sns.heatmap(conf,annot=True,fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#now checking the churn porbability for a specific client number by the model:Logistic Regression
dataset_check.head() #containing the CLIENTNUM column.

features_df.head() #containing all the features preprocessed or one-hot-encoded

#concatinating the dataset_check's CLIENTNUM column and features_df all columns into one dataframe with the name preprocessed_df.

# Concatenate along columns (axis=1)
preprocessed_df = pd.concat([dataset_check[['CLIENTNUM']], features_df], axis=1)

# Save the dataframe to a CSV for future reference
preprocessed_df.to_csv('preprocessed_df.csv', index=False)

preprocessed_df.head()

def churn_rate_pred(model_name):

  import joblib
  import random

  # Save the selected features used during model training
  imp_features = selected_features # List of 17 feature names used in training

  # Save the model
  joblib.dump(model_name, f'{model_name}_model.joblib')

  # Choose a random CLIENTNUM from the dataframe preprocessed_df
  random_client = random.choice(preprocessed_df['CLIENTNUM'])

  # Filter the row corresponding to the random CLIENTNUM
  client_row = preprocessed_df[preprocessed_df['CLIENTNUM'] == random_client]

  # Drop 'CLIENTNUM' and keep only the selected features
  features_client_num = client_row[imp_features]

  # Reshape the features to convert into a multi-dimensional array
  features_client_num = features_client_num.values.reshape(1, -1)

  #converting it to dataframe to avoid the warning
  features_client_num_df = pd.DataFrame(  features_client_num, columns = imp_features)
  features_client_num_scaled = scaler.transform(features_client_num_df)


  # Load the model from joblib
  model_log_reg = joblib.load(f'{model_name}_model.joblib')

  # Predict churn probability
  prediction = model_log_reg.predict_proba(features_client_num_scaled)
  print(f'Churn rate for CLIENTNUM {random_client}: {(prediction[0][1])*100:.2f}%')
  print(f'Not Churn rate for CLIENTNUM {random_client}: {(prediction[0][0])*100:.2f}%')

"""#Function 'churn_rate_pred(model_name)' will be used for predicting churn rate by other different models for future with the same processed dataset now with the name preprocessed_df"""

churn_rate_pred(logreg)

"""#Z-SCALING TECHNIQUE:
![image.png](data:image/png;base64,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)

#NAIVE-BAYES MODEL:
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib
import random


X = preprocessed_df.drop(columns=["CLIENTNUM", "Attrition_Flag"])
y = preprocessed_df["Attrition_Flag"]

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Save the model
joblib.dump(model, 'naive_bayes_model.joblib')

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

#confusion matrix
from sklearn.metrics import confusion_matrix
conf = confusion_matrix(y_test,y_pred)

#plotting heatmap
sns.heatmap(conf,annot=True,fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show

# Function to predict churn rate for a random CLIENTNUM
def churn_rate_pred_naive_bayes():
    # Save the selected features used during model training
    selected_features = X.columns.tolist()  # List of feature names used in training

    # Choose a random CLIENTNUM from the preprocessed DataFrame
    random_client = random.choice(preprocessed_df['CLIENTNUM'])

    # Filter the row corresponding to the random CLIENTNUM
    client_row = preprocessed_df[preprocessed_df['CLIENTNUM'] == random_client]

    # Drop 'CLIENTNUM' and keep only the selected features
    features_client_num = client_row[selected_features]

    # Reshape the features to convert into a multi-dimensional array
    features_client_num = features_client_num.values.reshape(1, -1)

    # Convert to DataFrame to avoid warnings
    features_client_num_df = pd.DataFrame(features_client_num, columns=selected_features)
    features_client_num_scaled = scaler.transform(features_client_num_df)

    # Load the model from joblib
    model_naive_bayes = joblib.load('naive_bayes_model.joblib')

    # Predict churn probability
    prediction = model_naive_bayes.predict_proba(features_client_num_scaled)
    print(f'Churn rate for CLIENTNUM {random_client}: {(prediction[0][1])*100:.2f}%')
    print(f'Not Churn rate for CLIENTNUM {random_client}: {(prediction[0][0])*100:.2f}%')

# Test the function
churn_rate_pred_naive_bayes()

"""#SUPPORT VECTOR MACHINE"""

preprocessed_df.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import joblib  # Import joblib to save the model and scaler

# Drop irrelevant columns (e.g., CLIENTNUM)
data = preprocessed_df.drop(columns=["CLIENTNUM"])

# Separate features and target variable
X = data.drop(columns=["Attrition_Flag"])
y = data["Attrition_Flag"]

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Initialize and fit the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data and transform it
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data using the same scaler
X_test_scaled = scaler.transform(X_test)

# Save the scaler to disk using joblib (so it can be reused later)
joblib.dump(scaler, 'scaler.joblib')

# Train SVM model
svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train_scaled, y_train)

# Save the trained model to disk
joblib.dump(svm_model, 'svm_model.joblib')

# Get prediction probabilities for Churned Class
y_prob = svm_model.predict_proba(X_test_scaled)[:, 1]
# 1 for positive class: Churned class

# Adjust threshold to remove bias/inclination towards one specific class
threshold = 0.6
y_pred = (y_prob >= threshold).astype(int)

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
print(f"SVM Accuracy: {accuracy}")
print("SVM Classification Report:\n", classification_report(y_test, y_pred))

"""#THRESHOLD ADJUSTMENT TO CONTROL BIASNESS:

By setting the threshold to 0.6, the model is being instructed to be stricter in assigning a sample to the positive class ("Churned"). Now, only probabilities greater than or equal to 0.6 will be classified as positive.
"""

# Optional: Confusion matrix and heatmap for better evaluation
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=["Not Churned", "Churned"], yticklabels=["Not Churned", "Churned"])
plt.title('Confusion Matrix')
plt.show()

# Load the saved scaler
scaler = joblib.load('scaler.joblib')

# Load the saved SVM model
svm_model = joblib.load('svm_model.joblib')

# Example of predicting for a random customer:
selected_features = X.columns.tolist()  # List of feature names used in training
random_client = random.choice(preprocessed_df['CLIENTNUM'])
client_row = preprocessed_df[preprocessed_df['CLIENTNUM'] == random_client]
features_client_num = client_row[selected_features].values.reshape(1, -1)

# Scale the features using the loaded scaler
features_client_num_scaled = scaler.transform(features_client_num)

# Get churn probability
churn_prob = svm_model.predict_proba(features_client_num_scaled)[:, 1]

print(f'Churn rate for CLIENTNUM {random_client}: {churn_prob[0]*100:.2f}%')
print(f'Not Churn rate for CLIENTNUM {random_client}: {(1 - churn_prob[0])*100:.2f}%')

!pip install gradio

!ls *.pkl

# import gradio as gr
# import pandas as pd
# import joblib
# import plotly.graph_objects as go

# # Save models
# joblib.dump(logreg, "logreg_model.joblib")
# model_naive_bayes = joblib.load('naive_bayes_model.joblib')
# joblib.dump(svm_model, "svm_model.joblib")

# # Load required files
# preprocessed_df = pd.read_csv("preprocessed_df.csv")  # Preprocessed dataset
# selected_features = joblib.load("selected_features.pkl")  # Selected features by RFE
# logreg_model = joblib.load("logreg_model.joblib")  # Logistic Regression model
# naive_bayes_model = joblib.load("naive_bayes_model.joblib")  # Naive Bayes model
# svm_model = joblib.load("svm_model.joblib")  # SVM model
# scaler = joblib.load("scaler.joblib")  # StandardScaler

# # Map CLIENTNUM to preprocessed data index
# clientnum_to_index = {num: idx for idx, num in enumerate(preprocessed_df['CLIENTNUM'])}

# # Prediction function
# def predict_churn_with_visuals(clientnum):
#     # Validate CLIENTNUM
#     if int(clientnum) not in clientnum_to_index:
#         return None, None

#     # Fetch preprocessed features for the client
#     customer_index = clientnum_to_index[int(clientnum)]
#     customer_features = preprocessed_df.iloc[customer_index][selected_features].values.reshape(1, -1)

#     # Scale the features
#     customer_features_scaled = scaler.transform(customer_features)

#     # Predict churn probability using all models
#     models = {
#         "Logistic Regression": logreg_model,
#         "Naive Bayes": naive_bayes_model,
#         "SVM": svm_model
#     }
#     churn_probs = []
#     for model in models.values():
#         pred = model.predict_proba(customer_features_scaled)
#         churn_probs.append(pred[0][1] * 100)

#     # Calculate average churn probability
#     avg_churn_prob = sum(churn_probs) / len(churn_probs)

#     # Create a gauge chart for churn probability
#     gauge_chart = go.Figure(go.Indicator(
#         mode="gauge+number",
#         value=avg_churn_prob,  # Use the average churn probability
#         title={"text": "Churn Probability", "font": {"color": "white"}},
#         gauge={
#             "axis": {"range": [0, 100], "tickwidth": 2, "tickcolor": "white"},
#             "bar": {"color": "rgba(200, 0, 0, 0.8)" if avg_churn_prob > 50 else "rgba(0, 128, 0, 0.8)"},  # Light red or green
#             "steps": [
#                 {"range": [0, 30], "color": "rgba(144, 238, 144, 0.5)", "thickness": 0.9},  # Light green
#                 {"range": [30, 70], "color": "rgba(255, 255, 102, 0.5)", "thickness": 0.9},  # Light yellow
#                 {"range": [70, 100], "color": "rgba(255, 99, 71, 0.5)", "thickness": 0.9}   # Light red
#             ],
#             "borderwidth": 2,  # Add border width for emphasis
#             "bordercolor": "rgba(255, 255, 255, 1)"  # Bright border for inner lining
#         }
#     ))
#     gauge_chart.update_layout(
#         paper_bgcolor="rgba(0,0,0,0)",  # Transparent background
#         font_color="white"
#     )

#     # Create a horizontal bar chart for model-wise probabilities
#     bar_chart = go.Figure(go.Bar(
#         x=churn_probs,
#         y=list(models.keys()),
#         orientation="h",
#         marker_color=["#1f77b4", "#ff7f0e", "#2ca02c"],  # Consistent bar colors
#         text=[f"{val:.2f}%" for val in churn_probs],  # Display percentages
#         textposition="outside"  # Place text outside the bars
#     ))
#     bar_chart.update_layout(
#       title="Churn Probability by Model",
#       xaxis=dict(
#           title="Probability (%)",
#           showgrid=False,
#           zeroline=False,
#           linecolor="white",
#           linewidth=0.5,  # Reduced from 1
#           range=[0, 110],  # Extends range slightly to show full percentages
#       ),
#       yaxis=dict(
#           title="Models",
#           showgrid=False,
#           zeroline=False,
#           linecolor="white",
#           linewidth=0.5,  # Reduced from 2
#       ),
#       plot_bgcolor="rgba(0,0,0,0)",
#       paper_bgcolor="rgba(0,0,0,0)",
#       font=dict(color="white", size=10),
#       margin=dict(r=20, t=40, b=40, l=20),  # Adjusted margins
#       bargap=0.3,  # Adjusted bar gap for better spacing
#   )

#   # Add text annotations for percentages
#     bar_chart.update_traces(
#         textposition='outside',
#         texttemplate='%{x:.1f}%',  # Format to show one decimal place
#         textfont=dict(color="white", size=10),
#         cliponaxis=False  # Ensures text is visible even if it extends beyond the plot area
#     )

#     return gauge_chart, bar_chart

# # Gradio UI
# def get_clientnum_dropdown():
#     """Generate a list of CLIENTNUMs for the dropdown."""
#     return preprocessed_df['CLIENTNUM'].astype(str).tolist()

# with gr.Blocks() as demo:
#     gr.Markdown("# Customer Churn Prediction Dashboard", elem_id="title")
#     gr.Markdown(
#         "Select a customer by their CLIENTNUM and click 'Predict Churn' to see the results.",
#         elem_id="description"
#     )

#     with gr.Row():
#         clientnum_dropdown = gr.Dropdown(
#             label="Select CLIENTNUM",
#             choices=get_clientnum_dropdown(),
#             value=get_clientnum_dropdown()[0]
#         )
#         predict_button = gr.Button("Predict Churn")

#     with gr.Row():
#         churn_gauge = gr.Plot(label="Average Churn Probability (Gauge Chart)")
#         churn_bar = gr.Plot(label="Churn Probability by Model")

#     # Define the interaction
#     predict_button.click(
#         predict_churn_with_visuals,
#         inputs=[clientnum_dropdown],
#         outputs=[churn_gauge, churn_bar]
#     )

# # Launch the Gradio app
# demo.launch()

preprocessed_df.shape

preprocessed_df.head()

import gradio as gr
import pandas as pd
import joblib
import plotly.graph_objects as go

# Load required files
preprocessed_df = pd.read_csv("preprocessed_df.csv")  # Preprocessed dataset
selected_features = joblib.load("selected_features.pkl")  # Selected features by RFE
logreg_model = joblib.load("logreg_model.joblib")  # Logistic Regression model
naive_bayes_model = joblib.load("naive_bayes_model.joblib")  # Naive Bayes model
svm_model = joblib.load("svm_model.joblib")  # SVM model
scaler = joblib.load("scaler.joblib")  # StandardScaler

# Map CLIENTNUM to preprocessed data index
clientnum_to_index = {num: idx for idx, num in enumerate(preprocessed_df['CLIENTNUM'])}

# Mappings for categorical columns
gender_mapping = {0: "Female", 1: "Male"}
education_level_mapping = {0: "College", 1: "Doctorate", 2: "Graduate", 3: "High School",
                           4: "Post-Graduate", 5: "Uneducated", 6: "Unknown"}
marital_status_mapping = {0: "Divorced", 1: "Married", 2: "Single", 3: "Unknown"}
income_category_mapping = {0: "$120K +", 1: "$40K - $60K", 2: "$60K - $80K", 3: "$80K - $120K",
                           4: "Less than $40K", 5: "Unknown"}
card_category_mapping = {0: "Blue", 1: "Gold", 2: "Platinum", 3: "Silver"}

# Prediction function
def predict_churn_with_visuals(clientnum):
    # Validate CLIENTNUM
    if int(clientnum) not in clientnum_to_index:
        return None, None

    # Fetch preprocessed features for the client
    customer_index = clientnum_to_index[int(clientnum)]
    customer_features = preprocessed_df.iloc[customer_index][selected_features].values.reshape(1, -1)

    # Scale the features
    customer_features_scaled = scaler.transform(customer_features)

    # Predict churn probability using all models
    models = {
        "Logistic Regression": logreg_model,
        "Naive Bayes": naive_bayes_model,
        "SVM": svm_model
    }
    churn_probs = []
    for model in models.values():
        pred = model.predict_proba(customer_features_scaled)
        churn_probs.append(pred[0][1] * 100)

    # Calculate average churn probability
    avg_churn_prob = sum(churn_probs) / len(churn_probs)

    # Create a gauge chart for churn probability
    gauge_chart = go.Figure(go.Indicator(
        mode="gauge+number",
        value=avg_churn_prob,  # Use the average churn probability
        title={"text": "Churn Probability", "font": {"color": "white"}},
        gauge={
            "axis": {"range": [0, 100], "tickwidth": 2, "tickcolor": "white"},
            "bar": {"color": "rgba(200, 0, 0, 0.8)" if avg_churn_prob > 50 else "rgba(0, 128, 0, 0.8)"},
            "steps": [
                {"range": [0, 30], "color": "rgba(144, 238, 144, 0.5)", "thickness": 0.9},  # Light green
                {"range": [30, 70], "color": "rgba(255, 255, 102, 0.5)", "thickness": 0.9},  # Light yellow
                {"range": [70, 100], "color": "rgba(255, 99, 71, 0.5)", "thickness": 0.9}   # Light red
            ],
            "borderwidth": 2,
            "bordercolor": "rgba(255, 255, 255, 1)"
        }
    ))
    gauge_chart.update_layout(
        paper_bgcolor="rgba(0,0,0,0)",
        font_color="white"
    )

    # Create a horizontal bar chart for model-wise probabilities
    bar_chart = go.Figure(go.Bar(
        x=churn_probs,
        y=list(models.keys()),
        orientation="h",
        marker_color=["#1f77b4", "#ff7f0e", "#2ca02c"],  # Consistent bar colors
        text=[f"{val:.2f}%" for val in churn_probs],  # Display percentages
        textposition="outside"  # Place text outside the bars
    ))
    bar_chart.update_layout(
        title="Churn Probability by Model",
        xaxis=dict(
            title="Probability (%)",
            showgrid=False,
            zeroline=False,
            linecolor="white",
            linewidth=0.5,
            range=[0, 110],
        ),
        yaxis=dict(
            title="Models",
            showgrid=False,
            zeroline=False,
            linecolor="white",
            linewidth=0.5,
        ),
        plot_bgcolor="rgba(0,0,0,0)",
        paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="white", size=10),
        margin=dict(r=20, t=40, b=40, l=20),
        bargap=0.3,
    )

    bar_chart.update_traces(
        textposition='outside',
        texttemplate='%{x:.1f}%',
        textfont=dict(color="white", size=10),
        cliponaxis=False
    )

    return gauge_chart, bar_chart

# Function to fetch customer info
def get_customer_info(clientnum):
    """Fetch customer details for the selected CLIENTNUM and map codes to original categories."""
    if int(clientnum) not in clientnum_to_index:
        return {}

    customer_index = clientnum_to_index[int(clientnum)]
    customer_data = preprocessed_df.iloc[customer_index]

    # Map encoded values to their corresponding categories
    gender = gender_mapping.get(customer_data["Gender"], "Unknown")
    education_level = education_level_mapping.get(customer_data["Education_Level"], "Unknown")
    marital_status = marital_status_mapping.get(customer_data["Marital_Status"], "Unknown")
    income_category = income_category_mapping.get(customer_data["Income_Category"], "Unknown")
    card_category = card_category_mapping.get(customer_data["Card_Category"], "Unknown")

    # Return customer info in a structured format
    return {
        "Age": customer_data["Customer_Age"],
        "Gender": gender,
        "Dependent Count": customer_data["Dependent_count"],
        "Education Level": education_level,
        "Marital Status": marital_status,
        "Income Category": income_category,
        "Card Category": card_category,
        "Months on Book": customer_data["Months_on_book"],
        "Total Relationships": customer_data["Total_Relationship_Count"],
        "Months Inactive": customer_data["Months_Inactive_12_mon"],
        "Contacts Count": customer_data["Contacts_Count_12_mon"],
        "Credit Limit": customer_data["Credit_Limit"],
        "Total Revolving Balance": customer_data["Total_Revolving_Bal"],
        "Avg Open to Buy": customer_data["Avg_Open_To_Buy"],
        "Total Transaction Amount": customer_data["Total_Trans_Amt"],
        "Total Transaction Count": customer_data["Total_Trans_Ct"],
        "Avg Utilization Ratio": customer_data["Avg_Utilization_Ratio"]
    }

# Gradio UI
def get_clientnum_dropdown():
    """Generate a list of CLIENTNUMs for the dropdown."""
    return preprocessed_df['CLIENTNUM'].astype(str).tolist()

with gr.Blocks() as demo:
    gr.Markdown("# Customer Churn Prediction Dashboard")
    gr.Markdown(
        "Select a customer by their CLIENTNUM and click 'Update Prediction' to see the results."
    )

    with gr.Row():
        clientnum_dropdown = gr.Dropdown(
            label="Select CLIENTNUM",
            choices=get_clientnum_dropdown(),
            value=get_clientnum_dropdown()[0]
        )
        update_button = gr.Button("Update Prediction")

    with gr.Row():
        with gr.Column():
            age = gr.Number(label="Age")
            gender = gr.Radio(
                label="Gender",
                choices=["Female", "Male"],
                value=None  # This will be dynamically updated below
            )
            dependent_count = gr.Number(label="Dependent Count")
            education_level = gr.Textbox(label="Education Level", value=None)
            marital_status = gr.Textbox(label="Marital Status", value=None)
        with gr.Column():
            income_category = gr.Textbox(label="Income Category", value=None)
            card_category = gr.Textbox(label="Card Category", value=None)
            months_on_book = gr.Number(label="Months on Book")
            total_relationships = gr.Number(label="Total Relationships")
            months_inactive = gr.Number(label="Months Inactive")
        with gr.Column():
            contacts_count = gr.Number(label="Contacts Count")
            credit_limit = gr.Number(label="Credit Limit")
            total_revolving_bal = gr.Number(label="Total Revolving Balance")
            avg_open_to_buy = gr.Number(label="Avg Open to Buy")
            total_trans_amt = gr.Number(label="Total Transaction Amount")
            total_trans_ct = gr.Number(label="Total Transaction Count")
            avg_utilization_ratio = gr.Number(label="Avg Utilization Ratio")

    with gr.Row():
        churn_gauge = gr.Plot(label="Average Churn Probability (Gauge Chart)")
        churn_bar = gr.Plot(label="Churn Probability by Model")

    # Combine customer info with predictions
    def display_customer_info_and_predict(clientnum):
        info = get_customer_info(clientnum)
        if not info:
            return [None] * 17 + [None, None]

        # Populate the fields
        fields = [
            info["Age"],
            info["Gender"],
            info["Dependent Count"],
            info["Education Level"],
            info["Marital Status"],
            info["Income Category"],
            info["Card Category"],
            info["Months on Book"],
            info["Total Relationships"],
            info["Months Inactive"],
            info["Contacts Count"],
            info["Credit Limit"],
            info["Total Revolving Balance"],
            info["Avg Open to Buy"],
            info["Total Transaction Amount"],
            info["Total Transaction Count"],
            info["Avg Utilization Ratio"]
        ]
        gauge_chart, bar_chart = predict_churn_with_visuals(clientnum)
        return fields + [gauge_chart, bar_chart]

    update_button.click(
        display_customer_info_and_predict,
        inputs=[clientnum_dropdown],
        outputs=[
            age, gender, dependent_count, education_level, marital_status,
            income_category, card_category, months_on_book, total_relationships,
            months_inactive, contacts_count, credit_limit, total_revolving_bal,
            avg_open_to_buy, total_trans_amt, total_trans_ct, avg_utilization_ratio,
            churn_gauge, churn_bar
        ]
    )

# Launch the Gradio app
demo.launch()

