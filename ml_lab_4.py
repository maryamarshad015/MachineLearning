# -*- coding: utf-8 -*-
"""ML LAB 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i4i2h21Z39uWX4cc3xCE7oJa5fcbA9qN

**ML-LAB 4**

**HOME-TASK:**

1. You are required to implement a linear regression model and a classification model
on two different datasets of your own choice following the procedures outlined in
the lab manual. The goal is to analyze the dataset, implement the model,
evaluate its performance, and visualize the results. You will need to document
each step, providing insights and analysis based on the findings.

**1.Linear-Regression Model**

*TikTok Dataset*
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from scipy import stats

# Load the TikTok dataset
file_path = 'tiktok_dataset.csv'  # Adjust to your file path
tiktok_df = pd.read_csv(file_path)

# Data Preprocessing
# Drop rows with missing values
cleaned_df = tiktok_df.dropna(subset=['video_view_count', 'video_like_count', 'video_share_count', 'video_comment_count'])

# Select relevant features for the models
features = ['video_duration_sec', 'video_view_count', 'video_share_count', 'video_comment_count', 'video_download_count']
X = cleaned_df[features]
y = cleaned_df['video_like_count']

# 1. Linear Regression Model
# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = linear_model.predict(X_test)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the results for Linear Regression
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

"""X_train consisting of features to be trained 80%
X_test consisting of features remaining 20% reserved for later testing

y_train consisting of 80% target variable upon which the model is trained
y_test consisting of 20% true output values for the test samples which the model will predict.

"""

# Visualize Actual vs Predicted
plt.figure(figsize=(5,3))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)
plt.xlabel("Actual Video Likes")
plt.ylabel("Predicted Video Likes")
plt.title("Linear Regression: Actual vs Predicted Likes")
plt.show()

# Visualize residuals
residuals = y_test - y_pred
plt.figure(figsize=(5,3))
sns.histplot(residuals, bins=20, kde=True, color='green')
plt.title('Residuals Distribution')
plt.show()

"""**Residual Value = Actual Value - Predicted Value by Model**

A huge spike at  0  shows that at most of the predicted values i.e for approximately 2200 videos the model has given good predictions as we are getting residual value of 0 for these much predicted videos.

**OVER-PREDICTION:**

But for residual value having -100000 it shows model has overpredicted the likes count for the videos.

*Negative Residual Value = Predicted Value > Actual Value*

**UNDER-PREDICTION:**

And similarly for the residual values of 100000,200000,300000,400000 the model has under-predicted the video like counts.

*Positive Residual Value = Predicted Value < Actual Value*
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""**2.Classification Model (Logistic-Regression Model)**

*BankChurners Dataset*
"""

# Load the dataset
df = pd.read_csv('BankChurners.csv')
df = df.drop(columns=['CLIENTNUM'])

# Perform one-hot encoding on categorical columns
df_encoded = pd.get_dummies(df, columns=['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category'], drop_first=True)

# Selecting features (excluding 'Attrition_Flag')
X = df_encoded.drop(columns=['Attrition_Flag'])
y = df_encoded['Attrition_Flag'].apply(lambda x: 1 if x == 'Attrited Customer' else 0)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build and train the Logistic Regression model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Predict and evaluate the model
y_pred = model.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

# Visualize confusion matrix
plt.figure(figsize=(5,3))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()