# -*- coding: utf-8 -*-
"""ML LAB 12 Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AK8AXbIPNZRtgj-ZlKH1grLQP6vNJD2N
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt


# Load dataset
df = pd.read_csv('heart_disease.csv')

"""#DATA PREPROCESSING"""

df.head()

df.info()

"""#Discovered some Missing values"""

#mean imputation for the missing values
cols = df[['cigsPerDay','BPMeds','totChol','BMI','heartRate','glucose','education']]
for col in cols:
    df[col].fillna(df[col].mean(), inplace=True)

df.isnull().sum()

df.describe()

"""#Feature Selection
#LASSOCV
"""

# Standardize the features for Lasso
X = df.drop(columns=['TenYearCHD'])
y = df['TenYearCHD']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply LassoCV
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_scaled, y)

# Select features with non-zero coefficients
selected_features = X.columns[lasso.coef_ != 0].tolist()
print("Selected Features from LassoCV:", selected_features)

"""#RANDOM FOREST"""

from imblearn.over_sampling import SMOTE
# Separate features and target
X = df.drop(columns=['TenYearCHD'])  # Features
y = df['TenYearCHD']                # Target

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Use selected features after SMOTE
X_resampled = X_resampled[selected_features]

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)

# Initialize models
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
log_reg = LogisticRegression(max_iter=1000, random_state=42)
svm_model = SVC(probability=True, random_state=42)

# Train models
rf_model.fit(X_train, y_train)
log_reg.fit(X_train, y_train)
svm_model.fit(X_train, y_train)

# Predictions and probabilities
rf_pred = rf_model.predict(X_test)
rf_probs = rf_model.predict_proba(X_test)[:, 1]

log_pred = log_reg.predict(X_test)
log_probs = log_reg.predict_proba(X_test)[:, 1]

svm_pred = svm_model.predict(X_test)
svm_probs = svm_model.predict_proba(X_test)[:, 1]

# Evaluation function
def evaluate_model(name, y_test, preds, probs):
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds)
    rec = recall_score(y_test, preds)
    roc_auc = roc_auc_score(y_test, probs)
    print(f"{name} Performance:")
    print(f"Accuracy: {acc:.2f}")
    print(f"Precision: {prec:.2f}")
    print(f"Recall: {rec:.2f}")
    print(f"ROC AUC: {roc_auc:.2f}")
    print("-" * 30)

# Evaluate each model
evaluate_model("Random Forest", y_test, rf_pred, rf_probs)

# Plot ROC curves
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
fpr_log, tpr_log, _ = roc_curve(y_test, log_probs)
fpr_svm, tpr_svm, _ = roc_curve(y_test, svm_probs)

plt.figure(figsize=(7,5))
plt.plot(fpr_rf, tpr_rf, label="Random Forest (AUC = {:.2f})".format(roc_auc_score(y_test, rf_probs)))
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()
plt.grid()
plt.show()

"""#What SMOTE Has Done to Your ROC-AUC Score?

The ROC-AUC score is a performance metric that evaluates the model's ability to distinguish between classes. By applying SMOTE, you essentially balance the dataset by generating synthetic samples for the minority class. This process can lead to:

#Improved AUC:

The ROC-AUC score is higher because the model is now better at distinguishing between the two classes. In the case of class imbalance, the model might have initially favored the majority class, leading to a poor ROC-AUC. SMOTE helps by giving the model more instances of the minority class, so it learns to better classify both classes.

#What is SMOTE?
SMOTE stands for Synthetic Minority Over-sampling Technique, and it is a popular technique used to handle class imbalance in machine learning. Here's how it works:

#1.Identifies Minority Class:
SMOTE identifies the minority class in the dataset (the class with fewer instances).

#2.Creates Synthetic Samples:
Instead of simply duplicating the minority class instances, SMOTE generates synthetic samples by creating new examples that are interpolated between existing minority class instances. Specifically:

For each sample in the minority class, SMOTE picks a random sample from its nearest neighbors.
A synthetic sample is then created by taking a point that lies between the original instance and its neighbor, based on a random selection of the neighbors.

#3.Resampling the Dataset:
This process effectively increases the number of minority class instances, which can help the classifier generalize better on the minority class and avoid biased predictions toward the majority class.
"""

#confusion_matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Confusion Matrix for Random Forest
rf_cm = confusion_matrix(y_test, rf_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='viridis', xticklabels=['No Heart Disease', 'Heart Disease'], yticklabels=['No Heart Disease', 'Heart Disease'])
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""#RANDOM FOREST VISUALIZATION:"""

from sklearn.tree import export_graphviz
from sklearn.ensemble import RandomForestClassifier
import pydotplus
from IPython.display import Image


# Export one tree from the Random Forest (e.g., the first tree)
tree = rf_model.estimators_[0]  # Selecting the first tree from the Random Forest

# Export the tree as DOT format (Graphviz)
dot_data = export_graphviz(tree, out_file=None,
                            feature_names=X[selected_features].columns,  # Use the actual feature names
                           class_names=['No CHD', 'CHD'],  # Change this based on your target labels
                           filled=True, rounded=True,
                           max_depth = 3,
                           special_characters=True)

# Create a graph from the DOT data
graph = pydotplus.graph_from_dot_data(dot_data)

# Save the graph as a PNG image
graph.write_png("random_forest_tree.png")

# Display the tree image
Image(graph.create_png())

"""#LOGISTIC REGRESSION:

#**With Similar 5 Features.**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score

# Logistic Regression using selected features
lr_model = LogisticRegression()
lr_model.fit(X_train[selected_features], y_train)  # Train using selected features
lr_pred = lr_model.predict(X_test[selected_features])  # Predict using selected features
lr_probs = lr_model.predict_proba(X_test[selected_features])[:, 1]  # Get probabilities for AUC
print("Logistic Regression ROC AUC Score:", roc_auc_score(y_test, lr_probs))
evaluate_model("Logistic Regression", y_test, lr_pred, lr_probs)

"""#CONFUSION MATRIX"""

#CONFUSION MATRIX
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Confusion Matrix for Logistic Regression
lr_cm = confusion_matrix(y_test, lr_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Heart Disease', 'Heart Disease'], yticklabels=['No Heart Disease', 'Heart Disease'])
plt.title('Confusion Matrix - Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""#ROC-AUC CURVE"""

# Plot the ROC curve
plt.figure(figsize=(7,5))
plt.plot(fpr_log, tpr_log, label="Logistic Regression (AUC = {:.2f})".format(roc_auc_score(y_test, lr_probs)))

# Plot random guess line
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess (AUC = 0.5)")

# Add labels and title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()
plt.grid()

# Show the plot
plt.show()

"""#SUPPORT VECTOR MACHINE:

#**With Similar 5 Features.**

#EVALUATION MATRICS
"""

# Support Vector Machine using selected features
svm_model = SVC(probability=True)
svm_model.fit(X_train[selected_features], y_train)  # Train using selected features
svm_probs = svm_model.predict_proba(X_test[selected_features])[:, 1]  # Get probabilities for AUC
print("SVM ROC AUC Score:", roc_auc_score(y_test, svm_probs))
evaluate_model("Support Vector Machine", y_test, svm_pred, svm_probs)

"""#CONFUSION MATRIX"""

#confusion Matrix

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Confusion Matrix for SVM
svm_cm = confusion_matrix(y_test, svm_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(svm_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Heart Disease', 'Heart Disease'], yticklabels=['No Heart Disease', 'Heart Disease'])

plt.title('Confusion Matrix - SVM')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""#ROC-AUC"""

# Plot the ROC curve
plt.figure(figsize=(7,5))
plt.plot(fpr_log, tpr_log, label="Support Vector Machine (AUC = {:.2f})".format(roc_auc_score(y_test, svm_probs)))

# Plot random guess line
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess (AUC = 0.5)")

# Add labels and title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()
plt.grid()

# Show the plot
plt.show()

"""#COMPARISON"""

plt.plot(fpr_rf, tpr_rf, label="Random Forest (AUC = {:.2f})".format(roc_auc_score(y_test, rf_probs)))

# Plot Logistic Regression ROC curve
plt.plot(fpr_log, tpr_log, label="Logistic Regression (AUC = {:.2f})".format(roc_auc_score(y_test, log_probs)))

# Plot SVM ROC curve
plt.plot(fpr_svm, tpr_svm, label="Support Vector Machine (AUC = {:.2f})".format(roc_auc_score(y_test, svm_probs)))

# Plot Random Guess Line (AUC = 0.5)
plt.plot([0, 1], [0, 1], 'k--', label="Random Guess (AUC = 0.5)")

# Add labels and title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Models')
plt.legend()
plt.grid()

# Show the plot
plt.show()

"""#RESEARCH TASK:

#Real-World Applications of Random Forest Classifiers
Random Forest classifiers have proven to be extremely versatile and effective across various industries. Below are three real-world applications of Random Forests, along with explanations of why they are particularly well-suited for these tasks.

#1. Healthcare Diagnosis and Prediction:
#Application:
 Predicting diseases such as heart disease, cancer, and diabetes.
#Why Random Forest?
#Handles Complex Data:
Healthcare data often involve complex, non-linear relationships between features like age, medical history, and lifestyle choices. Random Forest's ability to model these relationships makes it ideal.
#Robustness:
It works well with noisy, missing, and incomplete dataâ€”common in medical records.
#Feature Importance:
Random Forest can identify the most significant predictors for a disease, which can assist doctors in diagnosing and providing personalized treatment plans.
#Example:
Predicting whether a patient will develop heart disease based on risk factors.

#2. Financial Credit Scoring:
#Application:
Assessing the likelihood of a borrower defaulting on a loan.
#Why Random Forest?
Works Well with Mixed Data Types: It can handle both numerical data (income, credit score) and categorical data (marital status, job type).
#Accuracy:
Random Forests improve prediction accuracy by aggregating predictions from multiple decision trees, making them robust against overfitting.
#Transparency:
It provides insights into the importance of various financial features, which helps financial institutions refine their lending criteria.
#Example:
Predicting loan approval chances based on past financial behavior and demographic details.

#3. E-commerce and Customer Segmentation:
#Application:
Segmenting customers and personalizing recommendations.
#Why Random Forest?
Ability to Handle Unstructured Data: Random Forest can process both structured data (like purchase history) and unstructured data (like customer reviews).
#Non-linear Modeling:
It captures intricate patterns in customer behavior, which helps in personalizing product recommendations.
#Scalability:
Random Forest can efficiently handle large volumes of data, making it suitable for large-scale e-commerce platforms.
#Example:
Classifying customers into segments based on their buying habits to personalize marketing strategies.
"""















