# -*- coding: utf-8 -*-
"""Activity_Course 5 TikTok project lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fv-zr3U13SxRHJZsqdfMNndywtRyuay0

# **TikTok Project**
**Course 5 - Regression Analysis: Simplify complex data relationships**
"""

import numpy as np
import pandas as pd
import platform
import statsmodels
print('Python version: ', platform.python_version())
print('numpy version: ', np.__version__)
print('pandas version: ', pd.__version__)
print('statsmodels version: ', statsmodels.__version__)

"""You are a data professional at TikTok. The data team is working towards building a machine learning model that can be used to determine whether a video contains a claim or whether it offers an opinion. With a successful prediction model, TikTok can reduce the backlog of user reports and prioritize them more efficiently.

The team is getting closer to completing the project, having completed an initial plan of action, initial Python coding work, EDA, and hypothesis testing.

The TikTok team has reviewed the results of the hypothesis testing. TikTokâ€™s Operations Lead, Maika Abadi, is interested in how different variables are associated with whether a user is verified. Earlier, the data team observed that if a user is verified, they are much more likely to post opinions. Now, the data team has decided to explore how to predict verified status to help them understand how video characteristics relate to verified users. Therefore, you have been asked to conduct a logistic regression using verified status as the outcome variable. The results may be used to inform the final model related to predicting whether a video is a claim vs an opinion.

A notebook was structured and prepared to help you in this project. Please complete the following questions.

# **Course 5 End-of-course project: Regression modeling**


In this activity, you will build a logistic regression model in Python. As you have learned, logistic regression helps you estimate the probability of an outcome. For data science professionals, this is a useful skill because it allows you to consider more than one variable against the variable you're measuring against. This opens the door for much more thorough and flexible analysis to be completed.

<br/>

**The purpose** of this project is to demostrate knowledge of EDA and regression models.

**The goal** is to build a logistic regression model and evaluate the model.
<br/>
*This activity has three parts:*

**Part 1:** EDA & Checking Model Assumptions
* What are some purposes of EDA before constructing a logistic regression model?

**Part 2:** Model Building and Evaluation
* What resources do you find yourself using as you complete this stage?

**Part 3:** Interpreting Model Results

* What key insights emerged from your model(s)?

* What business recommendations do you propose based on the models built?

Follow the instructions and answer the question below to complete the activity. Then, you will complete an executive summary using the questions listed on the PACE Strategy Document.

Be sure to complete this activity before moving on. The next course item will provide you with a completed exemplar to compare to your own work.

# **Build a regression model**

<img src="images/Pace.png" width="100" height="100" align=left>

# **PACE stages**

Throughout these project notebooks, you'll see references to the problem-solving framework PACE. The following notebook components are labeled with the respective PACE stage: Plan, Analyze, Construct, and Execute.

<img src="images/Plan.png" width="100" height="100" align=left>


## **PACE: Plan**
Consider the questions in your PACE Strategy Document to reflect on the Plan stage.

### **Task 1. Imports and loading**
Import the data and packages that you've learned are needed for building regression models.
"""

# Import packages for data manipulation
import pandas as ps
import numpy as np

# Import packages for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Import packages for data preprocessing
import statsmodels.api as sm


# Import packages for data modeling
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

"""Load the TikTok dataset.

**Note:** As shown in this cell, the dataset has been automatically loaded in for you. You do not need to download the .csv file, or provide more code, in order to access the dataset and proceed with this lab. Please continue with this activity by completing the following instructions.
"""

# Load dataset into dataframe
data = pd.read_csv("tiktok_dataset.csv")

"""<img src="images/Analyze.png" width="100" height="100" align=left>

## **PACE: Analyze**

Consider the questions in your PACE Strategy Document to reflect on the Analyze stage.

In this stage, consider the following question where applicable to complete your code response:

* What are some purposes of EDA before constructing a logistic regression model?

==> ENTER YOUR RESPONSE HERE

### **Task 2a. Explore data with EDA**

Analyze the data and check for and handle missing values and duplicates.

Inspect the first five rows of the dataframe.
"""

# Display first few rows
data.head()

"""Get the number of rows and columns in the dataset."""

# Get number of rows and columns
data.shape

"""Get the data types of the columns."""

# Get data types of columns
data.info()

"""Get basic information about the dataset."""

# Get basic information
data.info()

"""Generate basic descriptive statistics about the dataset."""

# Generate basic descriptive stats
data.describe()

"""Check for and handle missing values."""

# Check for missing values
data.isnull().sum()

# Drop rows with missing values
data.dropna(axis=0,inplace=True)

data.isnull().sum()

# Display first few rows after handling missing values
data.head()

"""Check for and handle duplicates."""

# Check for duplicates
data.duplicated().sum()

"""Check for and handle outliers."""

# Create a boxplot to visualize distribution of `video_duration_sec`
plt.figure(figsize=(5,5))
sns.boxplot(data['video_duration_sec'])
plt.title('Video Duration (Sec)')

# Create a boxplot to visualize distribution of `video_view_count`
plt.figure(figsize=(5,4))
sns.boxplot(x=data['video_view_count'])
plt.xlabel('Video Views')

# Create a boxplot to visualize distribution of `video_like_count`
plt.figure(figsize=(5,4))
sns.boxplot(x=data['video_like_count'])
plt.xlabel('Video Likes')

# Create a boxplot to visualize distribution of `video_comment_count`
plt.figure(figsize=(5,4))
sns.boxplot(x=data['video_comment_count'])
plt.xlabel('Video Comment')

# Check for and handle outliers for video_like_count
q1 = data['video_like_count'].quantile(0.25)
q3 = data['video_like_count'].quantile(0.75)

iqr = q3 - q1

threshold = q3+ 1.5 * iqr

data.loc[data["video_like_count"] > threshold, "video_like_count"] = threshold

sns.boxplot(data["video_like_count"])

# Check for and handle outliers for video_like_count
q1 = data['video_comment_count'].quantile(0.25)
q3 = data['video_comment_count'].quantile(0.75)

iqr = q3 - q1

threshold = q3+ 1.5 * iqr

data.loc[data["video_comment_count"] > threshold, "video_comment_count"] = threshold

# Create a boxplot to visualize distribution of `video_comment_count`
plt.figure(figsize=(5,4))
sns.boxplot(x=data['video_comment_count'])
plt.xlabel('Video Comment')

"""Check class balance of the target variable. Remember, the goal is to predict whether the user of a given post is verified or unverified."""

# Check class balance
data['verified_status'].value_counts()

"""Approximately 94.2% of the dataset represents videos posted by unverified accounts and 5.8% represents videos posted by verified accounts. So the outcome variable is not very balanced.

Use resampling to create class balance in the outcome variable, if needed.
"""

# Use resampling to create class balance in the outcome variable, if needed
from sklearn.utils import resample


# Identify data points from majority and minority classes
majority_class = data[data['verified_status'] == 'not verified']
minority_class = data[data['verified_status'] == 'verified']


# Upsample the minority class (which is "verified")
minority_class =  resample(
    minority_class,
    replace = True,
    n_samples=len(majority_class),
    random_state = 42
)
# Combine majority class with upsampled minority class
balanced_df = pd.concat([majority_class,minority_class])
# Display new class counts
balanced_df['verified_status'].value_counts()

"""Get the average `video_transcription_text` length for videos posted by verified accounts and the average `video_transcription_text` length for videos posted by unverified accounts.


"""

data.claim_status.value_counts()

"""Extract the length of each `video_transcription_text` and add this as a column to the dataframe, so that it can be used as a potential feature in the model."""

# Get the average `video_transcription_text` length for claims and the average `video_transcription_text` length for opinions

balanced_df.groupby('claim_status')['video_transcription_text'].apply(lambda x: x.str.len().mean())

balanced_df.groupby('verified_status')['video_transcription_text'].apply(lambda x: x.str.len().mean())

# Extract the length of each `video_transcription_text` and add this as a column to the dataframe
balanced_df['video_transcription_text_len'] = balanced_df['video_transcription_text'].apply(func = lambda x: len(x))

balanced_df.head(3)

"""Visualize the distribution of `video_transcription_text` length for videos posted by verified accounts and videos posted by unverified accounts."""



# Visualize the distribution of `video_transcription_text` length for videos posted by verified accounts and videos posted by unverified accounts
# Create two histograms in one plot
sns.histplot(data=balanced_df, hue='verified_status', x='video_transcription_text_len', kde=True, palette = 'pastel', multiple='stack')
plt.title('Video Transcription TXT Length VS Verified Status')
plt.xlabel('Video Transcript Txt Length')

"""### **Task 2b. Examine correlations**

Next, code a correlation matrix to help determine most correlated variables.
"""

balanced_df.columns

# Code a correlation matrix to help determine most correlated variables
correlation_mat = balanced_df.corr(numeric_only=True)
correlation_mat

"""Visualize a correlation heatmap of the data."""

# Create a heatmap to visualize how correlated variables are
sns.heatmap(correlation_mat.drop(columns=['#']), annot=True, cmap='coolwarm',fmt='.2f')
plt.title('Correlation Matrix')

"""One of the model assumptions for logistic regression is no severe multicollinearity among the features. Take this into consideration as you examine the heatmap and choose which features to proceed with.

**Question:** What variables are shown to be correlated in the heatmap?


video_like_count is highly correlated with other variables like video_view_count , video_comment_count, video_download_count, video_share_count.
Hence excluding video_like_count would be a good decision as this feature vioaltes the assumption of multicollinearity which should not exist for logistic regression.

<img src="images/Construct.png" width="100" height="100" align=left>

## **PACE: Construct**

After analysis and deriving variables with close relationships, it is time to begin constructing the model. Consider the questions in your PACE Strategy Document to reflect on the Construct stage.

### **Task 3a. Select variables**

Set your Y and X variables.

Select the outcome variable.
"""

# Select outcome variable
y = balanced_df[['verified_status']]

balanced_df.head()

"""Select the features."""

# Select features
X = balanced_df.drop(columns=['verified_status','#','video_id','video_like_count','video_transcription_text','video_transcription_text_len'])


# Display first few rows of features dataframe
X.head()

"""### **Task 3b. Train-test split**

Split the data into training and testing sets.
"""

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=42)

"""Confirm that the dimensions of the training and testing sets are in alignment."""

# Get shape of each training and testing set
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""### **Task 3c. Encode variables**

Check the data types of the features.
"""

# Check data types
X_train.dtypes

# Get unique values in `claim_status`
X_train.claim_status.unique()

# Get unique values in `author_ban_status`
X_train.author_ban_status.unique()

"""As shown above, the `claim_status` and `author_ban_status` features are each of data type `object` currently. In order to work with the implementations of models through `sklearn`, these categorical features will need to be made numeric. One way to do this is through one-hot encoding.

Encode categorical features in the training set using an appropriate method.
"""

# Select the training features that needs to be encoded
X_train[['author_ban_status','claim_status']] = X_train[['author_ban_status','claim_status']].astype('category').apply(lambda x: x.cat.codes)

# Display first few rows
X_train.head()

X_train['author_ban_status'].unique()

y_train[['verified_status']]= y_train[['verified_status']].astype('category').apply(lambda x: x.cat.codes)

# Display first few rows
y_train[['verified_status']].head()

# Select the training features that needs to be encoded
X_test[['author_ban_status','claim_status']] = X_test[['author_ban_status','claim_status']].astype('category').apply(lambda x: x.cat.codes)

# Display first few rows
X_test.head()

y_test[['verified_status']] = y_test[['verified_status']].astype('category').apply(lambda x: x.cat.codes)

# Display first few rows
y_test.head()

X_test.columns

from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression(max_iter = 800, random_state=0 )
model = log_clf.fit(X_train,y_train.values.reshape(-1,1).ravel())

y_pred = model.predict(X_test)
y_pred

#confusion_matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, cmap='coolwarm',fmt='d')

plt.title('Confusion Matrix Heatmap')
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.show()

cm

from sklearn.metrics import classification_report

target_labels = ['verified', 'not verified']
print(classification_report(y_test,y_pred,target_names=target_labels))

"""<img src="images/Execute.png" width="100" height="100" align=left>

## **PACE: Execute**

Consider the questions in your PACE Strategy Document to reflect on the Execute stage.

### **Taks 4a. Results and evaluation**

Evaluate your model.

Encode categorical features in the testing set using an appropriate method.

### **Task 4b. Visualize model results**

Create a confusion matrix to visualize the results of the logistic regression model.
"""

#confusion_matrix

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, cmap='coolwarm',fmt='d')

plt.title('Confusion Matrix Heatmap')
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.show()

"""Create a classification report that includes precision, recall, f1-score, and accuracy metrics to evaluate the performance of the logistic regression model."""

from sklearn.metrics import classification_report

target_labels = ['verified', 'not verified']
print(classification_report(y_test,y_pred,target_names=target_labels))

"""### **Task 4c. Interpret model coefficients**"""

# Get the feature names from the model and the model coefficients (which represent log-odds ratios)
# Place into a DataFrame for readability
coeffecients = pd.DataFrame({
    'features':X_train.columns,
    'coeffecients':model.coef_[0],
})

coeffecients

"""### **Task 4d. Conclusion**

1. What are the key takeaways from this project?

2. What results can be presented from this project?

Exemplar response:

Key takeaways:

The dataset has a few strongly correlated variables, which might lead to multicollinearity issues when fitting a logistic regression model. We decided to drop video_like_count from the model building.
Based on the logistic regression model, each additional second of the video is associated with80.009 increase in the log-odds of the user having a verified status.
The logistic regression model had not great, but acceptable predictive power: a precision of 61% is less than ideal, but a recall of 84% is very good. Overall accuracy is towards the lower end of what would typically be considered acceptable.
We developed a logistic regression model for verified status based on video features. The model had decent predictive power. Based on the estimated model coefficients from the logistic regression, longer videos tend to be associated with higher odds of the user being verified. Other video features have small estimated coefficients in the model, so their association with verified status seems to be small.

**Congratulations!** You've completed this lab. However, you may not notice a green check mark next to this item on Coursera's platform. Please continue your progress regardless of the check mark. Just click on the "save" icon at the top of this notebook to ensure your work has been logged.
"""