# -*- coding: utf-8 -*-
"""ML LAB 11 Decsion Tress - ID3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17r7Sn5O668Is1UcGV86y419ABCqdhEyO

#Implementation of Decision Trees (ID3) Classifiers in Machine Learning
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

"""## Load the dataset

"""

file_path = 'CC GENERAL.csv'
data = pd.read_csv(file_path)

data.head(5)

"""#DESCRIPTION:

1. **CUST_ID**: Unique identifier for each customer.

2. **BALANCE**: The current balance amount on the credit card for a customer.

3. **BALANCE_FREQUENCY**: How often the balance is updated. A value close to 1 means it is frequently updated.

4. **PURCHASES**: The total dollar amount of purchases made by the customer using the credit card.

5. **ONEOFF_PURCHASES**: The total dollar amount of one-time purchases made by the customer.

6. **INSTALLMENTS_PURCHASES**: The total dollar amount of purchases made in installments.

7. **CASH_ADVANCE**: The total dollar amount of cash advances taken by the customer using the credit card.

8. **PURCHASES_FREQUENCY**: How often the customer makes purchases using the credit card (scaled between 0 and 1).

9. **ONEOFF_PURCHASES_FREQUENCY**: How often the customer makes one-time purchases (scaled between 0 and 1).

10. **PURCHASES_INSTALLMENTS_FREQUENCY**: How often the customer makes installment purchases (scaled between 0 and 1).

11. **CASH_ADVANCE_FREQUENCY**: How often the customer takes cash advances using the credit card (scaled between 0 and 1).

12. **CASH_ADVANCE_TRX**: The number of transactions involving cash advances.

13. **PURCHASES_TRX**: The number of purchase transactions made by the customer.

14. **CREDIT_LIMIT**: The maximum credit limit available to the customer.

15. **PAYMENTS**: The total dollar amount of payments made by the customer.

16. **MINIMUM_PAYMENTS**: The total dollar amount of the minimum payments made by the customer.

17. **PRC_FULL_PAYMENT**: The proportion of months where the customer paid the full balance (scaled between 0 and 1).

18. **TENURE**: The number of months the customer has been with the bank.
"""

data.describe()

data.info()

data.isnull().sum()

data.dropna().duplicated()

"""# Preprocessing

"""

# Handle missing values by filling them with the column mean (only for numeric columns)
numeric_cols = data.select_dtypes(include=np.number).columns
data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())

"""# Sample 100 rows for a smaller dataset

"""

data_sample = data.sample(n=100, random_state=42)

"""# Create a binary classification target:

"""

# Define 'High Spender' if BALANCE > median, otherwise 'Low Spender'
median_balance = data_sample['BALANCE'].median()
data_sample['Spender_Class'] = np.where(data_sample['BALANCE'] > median_balance, 'High', 'Low')

"""#What basically is median balance in dollars?"""

median_balance

data_sample.head()

"""# Drop unnecessary columns

"""

data_sample.drop(['CUST_ID', 'BALANCE'], axis=1, inplace=True)

"""# Encoding Categorical Columns

"""

data_sample['Spender_Class'] = pd.Categorical(data_sample['Spender_Class'])
data_sample['Spender_Class'] = data_sample['Spender_Class'].cat.codes

data_sample['Spender_Class'].head(5)

"""# Split the dataset

"""

X = data_sample.drop('Spender_Class', axis=1)
y = data_sample['Spender_Class']

"""# Train-test split

"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Build the decision tree with a restricted depth
To Reduce Complexity
"""

clf = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=10, random_state=42)  # max_depth = 5 to limit tree depth
clf.fit(X_train, y_train)

"""#WHAT IS ENTROPY AND INFORMATION GAIN?

#1. Entropy:
Entropy is a measure of uncertainty or impurity in a dataset.
It tells us how mixed the classes (categories) are in the data.
If all data points belong to the same class, entropy is 0 (pure dataset).
If data points are evenly split among classes, entropy is high (maximum uncertainty).

#Formula:
Entropy
(
ğ‘†
)
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘
ğ‘–
log
â¡
2
(
ğ‘
ğ‘–
)


Entropy(S)=âˆ’âˆ‘
i=1
n
â€‹
 p
i
â€‹
 log
2
â€‹
 (p
i
â€‹
 )

 Where:

ğ‘
ğ‘–
p
i
â€‹
  is the proportion of data points in class
ğ‘–
i.
ğ‘›

n is the number of classes.

#Example:
A dataset with 100 samples where all belong to "Class A" has
Entropy
=
0
Entropy=0 (no uncertainty).

A dataset with 50 samples in "Class A" and 50 in "Class B" has
Entropy
=
1
Entropy=1 (maximum uncertainty).

#2. Information Gain:
Information gain measures the reduction in entropy after splitting the dataset based on a feature.
It helps us decide which feature is the most informative for creating a decision boundary.

# Tree Visualization
"""

plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=X.columns, class_names=['Low', 'High'], filled=True, fontsize=8)
plt.title("Decision Tree using ID3 Algorithm (Limited Depth)")
plt.show()

"""Target variable in  dataset represents **high balance** (High) and **low balance** (Low), this decision tree illustrates how various features of the dataset (such as **Minimum Payments**, **Purchases TRX**, **Balance Frequency**, and **Credit Limit**) influence whether a customer belongs to the "High" or "Low" balance category.

Hereâ€™s what the tree represents in this context:

---

### **Root Node (Top Node):**
- **Feature**: `MINIMUM_PAYMENTS <= 445.276`
- **Entropy**: 0.998 (indicating high uncertainty in classifying the samples at this stage).
- **Samples**: 80 total (38 Low balance, 42 High balance).
- **Split**: This node splits customers based on their `Minimum Payments`. If a customer's minimum payment is less than or equal to 445.276, they go down the **True** branch; otherwise, they go down the **False** branch.

---

### **Left Branch (True):**
- **Feature**: `MINIMUM_PAYMENTS <= 257.903`
- **Samples**: 51 (9 Low balance, 42 High balance).
- **Entropy**: 0.672 (less uncertainty compared to the root node; most samples belong to the "High" class).
- **Interpretation**: Among customers with minimum payments â‰¤ 445.276, further division is made based on whether their payments are â‰¤ 257.903. This branch mostly represents **High balance customers**.

---

#### **Further Left Branch (Purchases TRX):**
- **Feature**: `PURCHASES_TRX <= 145.5`
- **Samples**: 34 (1 Low balance, 33 High balance).
- **Entropy**: 0.191 (very low uncertainty; most are High balance).
- **Interpretation**: Customers with fewer transactions (`Purchases TRX â‰¤ 145.5`) are highly likely to belong to the High balance class.

---

#### **Right Branch (Balance Frequency & Credit Limit):**
- **Feature**: `BALANCE_FREQUENCY <= 0.727` and `CREDIT_LIMIT <= 3000.0`
- **Samples**:
  - Balance Frequency: 17 (8 Low balance, 9 High balance).
  - Credit Limit: 13 (8 Low balance, 5 High balance).
- **Entropy**: Higher in these splits, indicating more uncertainty (classes are mixed here).
- **Interpretation**: These features help separate customers into High or Low balance groups. For example, customers with lower credit limits are more likely to belong to the **Low balance** group.

---

### **Right Branch (False):**
- **Entropy**: 0.0.
- **Samples**: 29 (29 Low balance, 0 High balance).
- **Interpretation**: Customers with `Minimum Payments > 445.276` belong exclusively to the Low balance class.

---

### **Key Insights from the Tree:**
1. **Minimum Payments** is the most important feature, as it appears at the root node and makes the initial split between High and Low balance customers.
2. **High balance customers** are generally associated with:
   - Lower minimum payments.
   - Fewer purchase transactions (`Purchases TRX`).
   - Higher balance frequencies (indicating consistent payments or usage).
   - Higher credit limits.
3. **Low balance customers** tend to have:
   - Higher minimum payments.
   - Lower balance frequencies.
   - Lower credit limits.

---

# Test the tree by classifying test instances
"""

y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)

# Calculate accuracy
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
print(f"Testing Accuracy: {test_accuracy * 100:.2f}%")

# Display the text representation of the tree
tree_rules = export_text(clf, feature_names=list(X.columns))
print(tree_rules)

# Test the tree by classifying test instances
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
# Calculate accuracy
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
print(f"Testing Accuracy: {test_accuracy * 100:.2f}%")

