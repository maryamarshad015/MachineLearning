# -*- coding: utf-8 -*-
"""ABC Algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GjmkDyXyRIa9M-hVZtcAGQ-NXHRAsWmo
"""

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ABC Algorithm Implementation
class ABCOptimizer:
    def __init__(self, n_employed=20, n_onlookers=20, max_iter=50, limit=10, bounds=None):
        self.n_employed = n_employed
        self.n_onlookers = n_onlookers
        self.max_iter = max_iter
        self.limit = limit
        self.bounds = bounds
        self.best_solution = None
        self.best_fitness = -np.inf
        self.fitness_history = []

    def initialize(self):
        self.population = np.random.uniform(low=self.bounds[:, 0], high=self.bounds[:, 1],
                                          size=(self.n_employed, len(self.bounds)))
        self.fitness = np.array([self.evaluate_fitness(sol) for sol in self.population])
        self.trials = np.zeros(self.n_employed)
        self.update_best()

    def evaluate_fitness(self, solution):
        # Convert continuous values to discrete parameters
        n_estimators = int(solution[0])
        max_depth = int(solution[1]) if solution[1] > 1 else None
        min_samples_split = int(solution[2])
        min_samples_leaf = int(solution[3])
        max_features = solution[4]

        # Create and evaluate model
        try:
            model = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                min_samples_split=min_samples_split,
                min_samples_leaf=min_samples_leaf,
                max_features=max_features,
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            return accuracy_score(y_test, y_pred)
        except:
            return 0  # Return 0 fitness for invalid parameter combinations

    def update_best(self):
        max_idx = np.argmax(self.fitness)
        if self.fitness[max_idx] > self.best_fitness:
            self.best_fitness = self.fitness[max_idx]
            self.best_solution = self.population[max_idx].copy()
        self.fitness_history.append(self.best_fitness)

    def employed_bee_phase(self):
        for i in range(self.n_employed):
            # Generate a new solution
            k = np.random.randint(self.n_employed)
            while k == i:
                k = np.random.randint(self.n_employed)
            j = np.random.randint(len(self.bounds))
            phi = np.random.uniform(-1, 1)
            new_solution = self.population[i].copy()
            new_solution[j] = self.population[i][j] + phi * (self.population[i][j] - self.population[k][j])

            # Apply bounds
            new_solution = np.clip(new_solution, self.bounds[:, 0], self.bounds[:, 1])

            # Evaluate and greedy selection
            new_fitness = self.evaluate_fitness(new_solution)
            if new_fitness > self.fitness[i]:
                self.population[i] = new_solution
                self.fitness[i] = new_fitness
                self.trials[i] = 0
            else:
                self.trials[i] += 1

    def onlooker_bee_phase(self):
        # Handle case where all fitness values are equal
        if np.all(self.fitness == self.fitness[0]):
            probabilities = np.ones(self.n_employed) / self.n_employed
        else:
            # Add small constant to avoid division by zero
            probabilities = (self.fitness - self.fitness.min() + 1e-10)
            probabilities /= probabilities.sum()

        for _ in range(self.n_onlookers):
            # Select a solution based on fitness
            i = np.random.choice(range(self.n_employed), p=probabilities)

            # Generate a new solution
            k = np.random.randint(self.n_employed)
            while k == i:
                k = np.random.randint(self.n_employed)
            j = np.random.randint(len(self.bounds))
            phi = np.random.uniform(-1, 1)
            new_solution = self.population[i].copy()
            new_solution[j] = self.population[i][j] + phi * (self.population[i][j] - self.population[k][j])

            # Apply bounds
            new_solution = np.clip(new_solution, self.bounds[:, 0], self.bounds[:, 1])

            # Evaluate and greedy selection
            new_fitness = self.evaluate_fitness(new_solution)
            if new_fitness > self.fitness[i]:
                self.population[i] = new_solution
                self.fitness[i] = new_fitness
                self.trials[i] = 0
            else:
                self.trials[i] += 1

    def scout_bee_phase(self):
        for i in range(self.n_employed):
            if self.trials[i] >= self.limit:
                self.population[i] = np.random.uniform(low=self.bounds[:, 0], high=self.bounds[:, 1], size=len(self.bounds))
                self.fitness[i] = self.evaluate_fitness(self.population[i])
                self.trials[i] = 0

    def optimize(self):
        self.initialize()
        for iteration in range(self.max_iter):
            self.employed_bee_phase()
            self.onlooker_bee_phase()
            self.update_best()
            self.scout_bee_phase()
            print(f"Iteration {iteration+1}, Best Fitness: {self.best_fitness:.4f}")
        return self.best_solution, self.best_fitness

# Define parameter bounds
# [n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features]
bounds = np.array([
    [10, 200],    # n_estimators
    [1, 20],      # max_depth
    [2, 20],      # min_samples_split
    [1, 20],      # min_samples_leaf
    [0.1, 0.999]  # max_features
])

# Run ABC optimization
abc = ABCOptimizer(n_employed=15, n_onlookers=15, max_iter=20, limit=5, bounds=bounds)
best_solution, best_fitness = abc.optimize()

# Convert best solution to parameters
best_params = {
    'n_estimators': int(best_solution[0]),
    'max_depth': int(best_solution[1]) if best_solution[1] > 1 else None,
    'min_samples_split': int(best_solution[2]),
    'min_samples_leaf': int(best_solution[3]),
    'max_features': best_solution[4]
}

print("\nBest Parameters Found:")
print(best_params)
print(f"Best Accuracy: {best_fitness:.4f}")

# Plot fitness progression
plt.figure(figsize=(10, 5))
plt.plot(abc.fitness_history)
plt.title('ABC Optimization Progress')
plt.xlabel('Iteration')
plt.ylabel('Best Accuracy')
plt.grid(True)
plt.show()

# Compare with default model
default_model = RandomForestClassifier(random_state=42)
default_model.fit(X_train, y_train)
y_pred_default = default_model.predict(X_test)
default_accuracy = accuracy_score(y_test, y_pred_default)

optimized_model = RandomForestClassifier(**best_params, random_state=42)
optimized_model.fit(X_train, y_train)
y_pred_opt = optimized_model.predict(X_test)
optimized_accuracy = accuracy_score(y_test, y_pred_opt)

print("\nPerformance Comparison:")
print(f"Default Model Accuracy: {default_accuracy:.4f}")
print(f"Optimized Model Accuracy: {optimized_accuracy:.4f}")
print(f"Improvement: {(optimized_accuracy - default_accuracy)*100:.2f}%")

